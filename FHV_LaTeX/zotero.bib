
@misc{oshea_introduction_2015,
  title     = {An {Introduction} to {Convolutional} {Neural} {Networks}},
  url       = {http://arxiv.org/abs/1511.08458},
  doi       = {10.48550/arXiv.1511.08458},
  abstract  = {The field of machine learning has taken a dramatic twist in recent times, with the rise of the Artificial Neural Network (ANN). These biologically inspired computational models are able to far exceed the performance of previous forms of artificial intelligence in common machine learning tasks. One of the most impressive forms of ANN architecture is that of the Convolutional Neural Network (CNN). CNNs are primarily used to solve difficult image-driven pattern recognition tasks and with their precise yet simple architecture, offers a simplified method of getting started with ANNs. This document provides a brief introduction to CNNs, discussing recently published papers and newly formed techniques in developing these brilliantly fantastic image recognition models. This introduction assumes you are familiar with the fundamentals of ANNs and machine learning.},
  urldate   = {2025-06-19},
  publisher = {arXiv},
  author    = {O'Shea, Keiron and Nash, Ryan},
  month     = dec,
  year      = {2015},
  note      = {arXiv:1511.08458},
  keywords  = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing}
}

@article{sherstinsky_fundamentals_2018,
  title     = {Fundamentals of {Recurrent} {Neural} {Network} ({RNN}) and {Long} {Short}-{Term} {Memory} ({LSTM}) {Network}},
  copyright = {arXiv.org perpetual, non-exclusive license},
  url       = {https://arxiv.org/abs/1808.03314},
  doi       = {10.48550/ARXIV.1808.03314},
  abstract  = {Because of their effectiveness in broad practical applications, LSTM networks have received a wealth of coverage in scientific journals, technical blogs, and implementation guides. However, in most articles, the inference formulas for the LSTM network and its parent, RNN, are stated axiomatically, while the training formulas are omitted altogether. In addition, the technique of "unrolling" an RNN is routinely presented without justification throughout the literature. The goal of this paper is to explain the essential RNN and LSTM fundamentals in a single document. Drawing from concepts in signal processing, we formally derive the canonical RNN formulation from differential equations. We then propose and prove a precise statement, which yields the RNN unrolling technique. We also review the difficulties with training the standard RNN and address them by transforming the RNN into the "Vanilla LSTM" network through a series of logical arguments. We provide all equations pertaining to the LSTM system together with detailed descriptions of its constituent entities. Albeit unconventional, our choice of notation and the method for presenting the LSTM system emphasizes ease of understanding. As part of the analysis, we identify new opportunities to enrich the LSTM system and incorporate these extensions into the Vanilla LSTM network, producing the most general LSTM variant to date. The target reader has already been exposed to RNNs and LSTM networks through numerous available resources and is open to an alternative pedagogical approach. A Machine Learning practitioner seeking guidance for implementing our new augmented LSTM model in software for experimentation and research will find the insights and derivations in this tutorial valuable as well.},
  urldate   = {2025-06-19},
  author    = {Sherstinsky, Alex},
  year      = {2018},
  keywords  = {FOS: Computer and information sciences, Machine Learning (cs.LG), Machine Learning (stat.ML)}
}

@article{sengar_generative_2024,
  title      = {Generative artificial intelligence: a systematic review and applications},
  issn       = {1573-7721},
  shorttitle = {Generative artificial intelligence},
  url        = {https://link.springer.com/10.1007/s11042-024-20016-1},
  doi        = {10.1007/s11042-024-20016-1},
  abstract   = {Abstract 
                In recent years, the study of artificial intelligence (AI) has undergone a paradigm shift. This has been propelled by the groundbreaking capabilities of generative models both in supervised and unsupervised learning scenarios. Generative AI has shown state-of-the-art performance in solving perplexing real-world conundrums in fields such as image translation, medical diagnostics, textual imagery fusion, natural language processing, and beyond. This paper documents the systematic review and analysis of recent advancements and techniques in Generative AI with a detailed discussion of their applications including application-specific models. Indeed, the major impact that generative AI has made to date, has been in language generation with the development of large language models, in the field of image translation and several other interdisciplinary applications of generative AI. Moreover, the primary contribution of this paper lies in its coherent synthesis of the latest advancements in these areas, seamlessly weaving together contemporary breakthroughs in the field. Particularly, how it shares an exploration of the future trajectory for generative AI. In conclusion, the paper ends with a discussion of Responsible AI principles, and the necessary ethical considerations for the sustainability and growth of these generative models.},
  language   = {en},
  urldate    = {2025-06-19},
  journal    = {Multimedia Tools and Applications},
  author     = {Sengar, Sandeep Singh and Hasan, Affan Bin and Kumar, Sanjay and Carroll, Fiona},
  month      = aug,
  year       = {2024}
}

@misc{huang_bidirectional_2015,
  title     = {Bidirectional {LSTM}-{CRF} {Models} for {Sequence} {Tagging}},
  url       = {http://arxiv.org/abs/1508.01991},
  doi       = {10.48550/arXiv.1508.01991},
  abstract  = {In this paper, we propose a variety of Long Short-Term Memory (LSTM) based models for sequence tagging. These models include LSTM networks, bidirectional LSTM (BI-LSTM) networks, LSTM with a Conditional Random Field (CRF) layer (LSTM-CRF) and bidirectional LSTM with a CRF layer (BI-LSTM-CRF). Our work is the first to apply a bidirectional LSTM CRF (denoted as BI-LSTM-CRF) model to NLP benchmark sequence tagging data sets. We show that the BI-LSTM-CRF model can efficiently use both past and future input features thanks to a bidirectional LSTM component. It can also use sentence level tag information thanks to a CRF layer. The BI-LSTM-CRF model can produce state of the art (or close to) accuracy on POS, chunking and NER data sets. In addition, it is robust and has less dependence on word embedding as compared to previous observations.},
  urldate   = {2025-05-25},
  publisher = {arXiv},
  author    = {Huang, Zhiheng and Xu, Wei and Yu, Kai},
  month     = aug,
  year      = {2015},
  note      = {arXiv:1508.01991},
  keywords  = {Computer Science - Computation and Language}
}

@misc{goodfellow_generative_2014,
  title     = {Generative {Adversarial} {Networks}},
  url       = {http://arxiv.org/abs/1406.2661},
  doi       = {10.48550/arXiv.1406.2661},
  abstract  = {We propose a new framework for estimating generative models via an adversarial process, in which we simultaneously train two models: a generative model G that captures the data distribution, and a discriminative model D that estimates the probability that a sample came from the training data rather than G. The training procedure for G is to maximize the probability of D making a mistake. This framework corresponds to a minimax two-player game. In the space of arbitrary functions G and D, a unique solution exists, with G recovering the training data distribution and D equal to 1/2 everywhere. In the case where G and D are defined by multilayer perceptrons, the entire system can be trained with backpropagation. There is no need for any Markov chains or unrolled approximate inference networks during either training or generation of samples. Experiments demonstrate the potential of the framework through qualitative and quantitative evaluation of the generated samples.},
  urldate   = {2025-05-25},
  publisher = {arXiv},
  author    = {Goodfellow, Ian J. and Pouget-Abadie, Jean and Mirza, Mehdi and Xu, Bing and Warde-Farley, David and Ozair, Sherjil and Courville, Aaron and Bengio, Yoshua},
  month     = jun,
  year      = {2014},
  note      = {arXiv:1406.2661},
  keywords  = {Computer Science - Machine Learning, Statistics - Machine Learning}
}

@inproceedings{koch_siamese_2015,
  title    = {Siamese {Neural} {Networks} for {One}-shot {Image} {Recognition}},
  author   = {Koch, Gregory and Zemel, Richard and Salakhutdinov, Ruslan},
  year     = {2015},
  keywords = {neural-networks one-shot-learning}
}

@inproceedings{zi_style_2022,
  title     = {Style {Change} {Detection} {Based} {On} {Bi}-{LSTM} {And} {Bert}},
  url       = {https://api.semanticscholar.org/CorpusID:251471634},
  booktitle = {Conference and {Labs} of the {Evaluation} {Forum}},
  author    = {Zi, Jia and Zhou, Ling and Liu, Zheng Yi},
  year      = {2022}
}

@inproceedings{zeiler_deconvolutional_2010,
  title     = {Deconvolutional networks},
  doi       = {10.1109/CVPR.2010.5539957},
  booktitle = {2010 {IEEE} {Computer} {Society} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
  author    = {Zeiler, Matthew D. and Krishnan, Dilip and Taylor, Graham W. and Fergus, Rob},
  year      = {2010},
  keywords  = {Computer architecture, Convolution, Decoding, Feature extraction, Filters, Image edge detection, Image representation, Image restoration, Object recognition, Robustness},
  pages     = {2528--2535}
}

@misc{sung_learning_2018,
  title      = {Learning to {Compare}: {Relation} {Network} for {Few}-{Shot} {Learning}},
  shorttitle = {Learning to {Compare}},
  url        = {http://arxiv.org/abs/1711.06025},
  doi        = {10.48550/arXiv.1711.06025},
  abstract   = {We present a conceptually simple, flexible, and general framework for few-shot learning, where a classifier must learn to recognise new classes given only few examples from each. Our method, called the Relation Network (RN), is trained end-to-end from scratch. During meta-learning, it learns to learn a deep distance metric to compare a small number of images within episodes, each of which is designed to simulate the few-shot setting. Once trained, a RN is able to classify images of new classes by computing relation scores between query images and the few examples of each new class without further updating the network. Besides providing improved performance on few-shot learning, our framework is easily extended to zero-shot learning. Extensive experiments on five benchmarks demonstrate that our simple approach provides a unified and effective approach for both of these two tasks.},
  urldate    = {2025-05-21},
  publisher  = {arXiv},
  author     = {Sung, Flood and Yang, Yongxin and Zhang, Li and Xiang, Tao and Torr, Philip H. S. and Hospedales, Timothy M.},
  month      = mar,
  year       = {2018},
  note       = {arXiv:1711.06025},
  keywords   = {Computer Science - Computer Vision and Pattern Recognition}
}

@misc{faghri_vse_2018,
  title      = {{VSE}++: {Improving} {Visual}-{Semantic} {Embeddings} with {Hard} {Negatives}},
  shorttitle = {{VSE}++},
  url        = {http://arxiv.org/abs/1707.05612},
  doi        = {10.48550/arXiv.1707.05612},
  abstract   = {We present a new technique for learning visual-semantic embeddings for cross-modal retrieval. Inspired by hard negative mining, the use of hard negatives in structured prediction, and ranking loss functions, we introduce a simple change to common loss functions used for multi-modal embeddings. That, combined with fine-tuning and use of augmented data, yields significant gains in retrieval performance. We showcase our approach, VSE++, on MS-COCO and Flickr30K datasets, using ablation studies and comparisons with existing methods. On MS-COCO our approach outperforms state-of-the-art methods by 8.8\% in caption retrieval and 11.3\% in image retrieval (at R@1).},
  urldate    = {2025-05-21},
  publisher  = {arXiv},
  author     = {Faghri, Fartash and Fleet, David J. and Kiros, Jamie Ryan and Fidler, Sanja},
  month      = jul,
  year       = {2018},
  note       = {arXiv:1707.05612},
  keywords   = {Computer Science - Computation and Language, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning}
}

@misc{upgrad_overview_2023,
  title   = {An overview on multilayer perceptron ({MLP}) in machine learning},
  url     = {https://www.upgrad.com/blog/multilayer-perceptron-mlp-in-machine-learning/},
  urldate = {2025-05-21},
  author  = {{upGrad}},
  month   = jun,
  year    = {2023}
}

@misc{he_deep_2015,
  title     = {Deep {Residual} {Learning} for {Image} {Recognition}},
  url       = {http://arxiv.org/abs/1512.03385},
  doi       = {10.48550/arXiv.1512.03385},
  abstract  = {Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers---8x deeper than VGG nets but still having lower complexity. An ensemble of these residual nets achieves 3.57\% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28\% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC \& COCO 2015 competitions, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.},
  urldate   = {2025-05-21},
  publisher = {arXiv},
  author    = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  month     = dec,
  year      = {2015},
  note      = {arXiv:1512.03385},
  keywords  = {Computer Science - Computer Vision and Pattern Recognition}
}

@book{brownlee_deep_2019,
  title     = {Deep {Learning} for {Computer} {Vision}: {Image} {Classification}, {Object} {Detection}, and {Face} {Recognition} in {Python}},
  url       = {https://books.google.at/books?id=DOamDwAAQBAJ},
  publisher = {Machine Learning Mastery},
  author    = {Brownlee, J.},
  year      = {2019}
}

@book{muller_introduction_2017,
  address    = {Sebastopol, CA},
  edition    = {First edition},
  title      = {Introduction to machine learning with {Python}: a guide for data scientists},
  isbn       = {9781449369415},
  shorttitle = {Introduction to machine learning with {Python}},
  language   = {eng},
  publisher  = {O'Reilly Media},
  author     = {Müller, Andreas Christian and Guido, Sarah},
  year       = {2017}
}

@inproceedings{song_gp-bpr_2019,
  address    = {Nice France},
  title      = {{GP}-{BPR}: {Personalized} {Compatibility} {Modeling} for {Clothing} {Matching}},
  isbn       = {9781450368896},
  shorttitle = {{GP}-{BPR}},
  url        = {https://dl.acm.org/doi/10.1145/3343031.3350956},
  doi        = {10.1145/3343031.3350956},
  language   = {en},
  urldate    = {2025-05-16},
  booktitle  = {Proceedings of the 27th {ACM} {International} {Conference} on {Multimedia}},
  publisher  = {ACM},
  author     = {Song, Xuemeng and Han, Xianjing and Li, Yunkai and Chen, Jingyuan and Xu, Xin-Shun and Nie, Liqiang},
  month      = oct,
  year       = {2019},
  pages      = {320--328}
}

@misc{ge_deepfashion2_2019,
  title      = {{DeepFashion2}: {A} {Versatile} {Benchmark} for {Detection}, {Pose} {Estimation}, {Segmentation} and {Re}-{Identification} of {Clothing} {Images}},
  shorttitle = {{DeepFashion2}},
  url        = {http://arxiv.org/abs/1901.07973},
  doi        = {10.48550/arXiv.1901.07973},
  abstract   = {Understanding fashion images has been advanced by benchmarks with rich annotations such as DeepFashion, whose labels include clothing categories, landmarks, and consumer-commercial image pairs. However, DeepFashion has nonnegligible issues such as single clothing-item per image, sparse landmarks (4{\textasciitilde}8 only), and no per-pixel masks, making it had significant gap from real-world scenarios. We fill in the gap by presenting DeepFashion2 to address these issues. It is a versatile benchmark of four tasks including clothes detection, pose estimation, segmentation, and retrieval. It has 801K clothing items where each item has rich annotations such as style, scale, viewpoint, occlusion, bounding box, dense landmarks and masks. There are also 873K Commercial-Consumer clothes pairs. A strong baseline is proposed, called Match R-CNN, which builds upon Mask R-CNN to solve the above four tasks in an end-to-end manner. Extensive evaluations are conducted with different criterions in DeepFashion2.},
  urldate    = {2025-05-16},
  publisher  = {arXiv},
  author     = {Ge, Yuying and Zhang, Ruimao and Wu, Lingyun and Wang, Xiaogang and Tang, Xiaoou and Luo, Ping},
  month      = jan,
  year       = {2019},
  note       = {arXiv:1901.07973},
  keywords   = {Computer Science - Computer Vision and Pattern Recognition}
}

@article{pan_drag_2023,
  title      = {Drag {Your} {GAN}: {Interactive} {Point}-based {Manipulation} on the {Generative} {Image} {Manifold}},
  copyright  = {Creative Commons Attribution 4.0 International},
  shorttitle = {Drag {Your} {GAN}},
  url        = {https://arxiv.org/abs/2305.10973},
  doi        = {10.48550/ARXIV.2305.10973},
  abstract   = {Synthesizing visual content that meets users' needs often requires flexible and precise controllability of the pose, shape, expression, and layout of the generated objects. Existing approaches gain controllability of generative adversarial networks (GANs) via manually annotated training data or a prior 3D model, which often lack flexibility, precision, and generality. In this work, we study a powerful yet much less explored way of controlling GANs, that is, to "drag" any points of the image to precisely reach target points in a user-interactive manner, as shown in Fig.1. To achieve this, we propose DragGAN, which consists of two main components: 1) a feature-based motion supervision that drives the handle point to move towards the target position, and 2) a new point tracking approach that leverages the discriminative generator features to keep localizing the position of the handle points. Through DragGAN, anyone can deform an image with precise control over where pixels go, thus manipulating the pose, shape, expression, and layout of diverse categories such as animals, cars, humans, landscapes, etc. As these manipulations are performed on the learned generative image manifold of a GAN, they tend to produce realistic outputs even for challenging scenarios such as hallucinating occluded content and deforming shapes that consistently follow the object's rigidity. Both qualitative and quantitative comparisons demonstrate the advantage of DragGAN over prior approaches in the tasks of image manipulation and point tracking. We also showcase the manipulation of real images through GAN inversion.},
  urldate    = {2025-05-16},
  author     = {Pan, Xingang and Tewari, Ayush and Leimkühler, Thomas and Liu, Lingjie and Meka, Abhimitra and Theobalt, Christian},
  year       = {2023},
  keywords   = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, Graphics (cs.GR)}
}

@inproceedings{goenka_fashionvlp_2022,
  title     = {{FashionVLP}: {Vision} {Language} {Transformer} for {Fashion} {Retrieval} {With} {Feedback}},
  booktitle = {Proceedings of the {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
  author    = {Goenka, Sonam and Zheng, Zhaoheng and Jaiswal, Ayush and Chada, Rakesh and Wu, Yue and Hedau, Varsha and Natarajan, Pradeep},
  month     = jun,
  year      = {2022},
  pages     = {14105--14115}
}

@misc{huang_fashionfae_2024,
  title      = {{FashionFAE}: {Fine}-grained {Attributes} {Enhanced} {Fashion} {Vision}-{Language} {Pre}-training},
  copyright  = {Creative Commons Attribution Non Commercial Share Alike 4.0 International},
  shorttitle = {{FashionFAE}},
  url        = {https://arxiv.org/abs/2412.19997},
  doi        = {10.48550/ARXIV.2412.19997},
  abstract   = {Large-scale Vision-Language Pre-training (VLP) has demonstrated remarkable success in the general domain. However, in the fashion domain, items are distinguished by fine-grained attributes like texture and material, which are crucial for tasks such as retrieval. Existing models often fail to leverage these fine-grained attributes from both text and image modalities. To address the above issues, we propose a novel approach for the fashion domain, Fine-grained Attributes Enhanced VLP (FashionFAE), which focuses on the detailed characteristics of fashion data. An attribute-emphasized text prediction task is proposed to predict fine-grained attributes of the items. This forces the model to focus on the salient attributes from the text modality. Additionally, a novel attribute-promoted image reconstruction task is proposed, which further enhances the fine-grained ability of the model by leveraging the representative attributes from the image modality. Extensive experiments show that FashionFAE significantly outperforms State-Of-The-Art (SOTA) methods, achieving 2.9\% and 5.2\% improvements in retrieval on sub-test and full test sets, respectively, and a 1.6\% average improvement in recognition tasks.},
  urldate    = {2025-04-17},
  publisher  = {arXiv},
  author     = {Huang, Jiale and Gao, Dehong and Zhang, Jinxia and Zhan, Zechao and Hu, Yang and Wang, Xin},
  year       = {2024},
  keywords   = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences}
}

@inproceedings{song_neurostylist_2017,
  address    = {Mountain View California USA},
  title      = {{NeuroStylist}: {Neural} {Compatibility} {Modeling} for {Clothing} {Matching}},
  isbn       = {9781450349062},
  shorttitle = {{NeuroStylist}},
  url        = {https://dl.acm.org/doi/10.1145/3123266.3123314},
  doi        = {10.1145/3123266.3123314},
  language   = {en},
  urldate    = {2025-04-17},
  booktitle  = {Proceedings of the 25th {ACM} international conference on {Multimedia}},
  publisher  = {ACM},
  author     = {Song, Xuemeng and Feng, Fuli and Liu, Jinhuan and Li, Zekun and Nie, Liqiang and Ma, Jun},
  month      = oct,
  year       = {2017},
  pages      = {753--761}
}

@article{song_modality-oriented_2023,
  title     = {Modality-{Oriented} {Graph} {Learning} {Toward} {Outfit} {Compatibility} {Modeling}},
  volume    = {25},
  copyright = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html},
  issn      = {1520-9210, 1941-0077},
  url       = {https://ieeexplore.ieee.org/document/9645182/},
  doi       = {10.1109/TMM.2021.3134164},
  urldate   = {2025-04-17},
  journal   = {IEEE Transactions on Multimedia},
  author    = {Song, Xuemeng and Fang, Shi-Ting and Chen, Xiaolin and Wei, Yinwei and Zhao, Zhongzhou and Nie, Liqiang},
  year      = {2023},
  pages     = {856--867}
}

@inproceedings{pang_learning_2024,
  address   = {Waikoloa, HI, USA},
  title     = {Learning {Visual} {Body}-shape-{Aware} {Embeddings} for {Fashion} {Compatibility}},
  copyright = {https://doi.org/10.15223/policy-029},
  isbn      = {9798350318920},
  url       = {https://ieeexplore.ieee.org/document/10484117/},
  doi       = {10.1109/WACV57701.2024.00787},
  urldate   = {2025-04-17},
  booktitle = {2024 {IEEE}/{CVF} {Winter} {Conference} on {Applications} of {Computer} {Vision} ({WACV})},
  publisher = {IEEE},
  author    = {Pang, Kaicheng and Zou, Xingxing and Wong, Waikeung},
  month     = jan,
  year      = {2024},
  pages     = {8041--8050}
}

@article{meng_fashion_2023,
  title     = {Fashion {Outfit} {Recommendation} and {Transformation} {Using} {Generative} {Adversarial} {Network} {Methods}},
  volume    = {41},
  copyright = {https://creativecommons.org/licenses/by-nc/4.0},
  issn      = {2791-0210},
  url       = {https://drpress.org/ojs/index.php/HSET/article/view/6809},
  doi       = {10.54097/hset.v41i.6809},
  abstract  = {Over the last few decades, generative adversarial network (GAN) methods have been increasingly applied and improved in fashion-related scenes, especially during the pandemic. This article provides an overview of several mainstream GAN models for fashion recommendation and fashion transformation, including compatibility matrix-regularized GAN, Outfit GAN, transformer-based GAN, conditional analogy GAN, disentangled cycle-consistency try-on network, and flow-navigated warping GAN. Within each GAN method, the improvements made to perform more related and deliciated are presented with detailed loss functions, the specific evaluation matrixes, and the examinations of generators or discriminators. Moreover, possible directions of improvements for future research and problems with existing models are involved. As for the common restrictions of included methods, including the problem of incomprehensive and non-diverse datasets, the standard evaluation of modish aesthetic value, and the number of fashion items involved in one outfit, are analyzed within fashion outfit recommendation and fashion transformation. Therefore, such restrictions provide possible directions for future analysis in fashion-related networks.},
  urldate   = {2025-04-17},
  journal   = {Highlights in Science, Engineering and Technology},
  author    = {Meng, Xinyi},
  month     = mar,
  year      = {2023},
  pages     = {192--201}
}

@misc{banerjee_recommendation_2022,
  title     = {Recommendation of {Compatible} {Outfits} {Conditioned} on {Style}},
  copyright = {Creative Commons Attribution 4.0 International},
  url       = {https://arxiv.org/abs/2203.16161},
  doi       = {10.48550/ARXIV.2203.16161},
  abstract  = {Recommendation in the fashion domain has seen a recent surge in research in various areas, for example, shop-the-look, context-aware outfit creation, personalizing outfit creation, etc. The majority of state of the art approaches in the domain of outfit recommendation pursue to improve compatibility among items so as to produce high quality outfits. Some recent works have realized that style is an important factor in fashion and have incorporated it in compatibility learning and outfit generation. These methods often depend on the availability of fine-grained product categories or the presence of rich item attributes (e.g., long-skirt, mini-skirt, etc.). In this work, we aim to generate outfits conditional on styles or themes as one would dress in real life, operating under the practical assumption that each item is mapped to a high level category as driven by the taxonomy of an online portal, like outdoor, formal etc and an image. We use a novel style encoder network that renders outfit styles in a smooth latent space. We present an extensive analysis of different aspects of our method and demonstrate its superiority over existing state of the art baselines through rigorous experiments.},
  urldate   = {2025-04-17},
  publisher = {arXiv},
  author    = {Banerjee, Debopriyo and Dhakad, Lucky and Maheshwari, Harsh and Chelliah, Muthusamy and Ganguly, Niloy and Bhattacharya, Arnab},
  year      = {2022},
  keywords  = {Artificial Intelligence (cs.AI), Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, Information Retrieval (cs.IR), Machine Learning (cs.LG)}
}

@article{guan_partially_2022,
  title     = {Partially {Supervised} {Compatibility} {Modeling}},
  volume    = {31},
  copyright = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html},
  issn      = {1057-7149, 1941-0042},
  url       = {https://ieeexplore.ieee.org/document/9817021/},
  doi       = {10.1109/TIP.2022.3187290},
  urldate   = {2025-04-17},
  journal   = {IEEE Transactions on Image Processing},
  author    = {Guan, Weili and Wen, Haokun and Song, Xuemeng and Wang, Chun and Yeh, Chung-Hsing and Chang, Xiaojun and Nie, Liqiang},
  year      = {2022},
  pages     = {4733--4745}
}

@misc{polania_learning_2019,
  title     = {Learning fashion compatibility across apparel categories for outfit recommendation},
  copyright = {arXiv.org perpetual, non-exclusive license},
  url       = {https://arxiv.org/abs/1905.03703},
  doi       = {10.48550/ARXIV.1905.03703},
  abstract  = {This paper addresses the problem of generating recommendations for completing the outfit given that a user is interested in a particular apparel item. The proposed method is based on a siamese network used for feature extraction followed by a fully-connected network used for learning a fashion compatibility metric. The embeddings generated by the siamese network are augmented with color histogram features motivated by the important role that color plays in determining fashion compatibility. The training of the network is formulated as a maximum a posteriori (MAP) problem where Laplacian distributions are assumed for the filters of the siamese network to promote sparsity and matrix-variate normal distributions are assumed for the weights of the metric network to efficiently exploit correlations between the input units of each fully-connected layer.},
  urldate   = {2025-04-17},
  publisher = {arXiv},
  author    = {Polania, Luisa F. and Gupte, Satyajit},
  year      = {2019},
  keywords  = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences}
}

@inproceedings{li_prediction_2022,
  address    = {Guangzhou, China},
  title      = {Prediction of {Outfit} {Compatibility} {Based} on {Weighted} {Multimodal} {Fusion}:},
  shorttitle = {Prediction of {Outfit} {Compatibility} {Based} on {Weighted} {Multimodal} {Fusion}},
  url        = {https://www.atlantis-press.com/article/125982793},
  doi        = {10.2991/978-94-6463-108-1_2},
  urldate    = {2025-04-17},
  author     = {Li, Yan and Fu, Xiaobao and Du, You},
  year       = {2022}
}

@article{zhou_learning_2025,
  title      = {Learning to {Synthesize} {Compatible} {Fashion} {Items} {Using} {Semantic} {Alignment} and {Collocation} {Classification}: {An} {Outfit} {Generation} {Framework}},
  copyright  = {arXiv.org perpetual, non-exclusive license},
  shorttitle = {Learning to {Synthesize} {Compatible} {Fashion} {Items} {Using} {Semantic} {Alignment} and {Collocation} {Classification}},
  url        = {https://arxiv.org/abs/2502.06827},
  doi        = {10.48550/ARXIV.2502.06827},
  abstract   = {The field of fashion compatibility learning has attracted great attention from both the academic and industrial communities in recent years. Many studies have been carried out for fashion compatibility prediction, collocated outfit recommendation, artificial intelligence (AI)-enabled compatible fashion design, and related topics. In particular, AI-enabled compatible fashion design can be used to synthesize compatible fashion items or outfits in order to improve the design experience for designers or the efficacy of recommendations for customers. However, previous generative models for collocated fashion synthesis have generally focused on the image-to-image translation between fashion items of upper and lower clothing. In this paper, we propose a novel outfit generation framework, i.e., OutfitGAN, with the aim of synthesizing a set of complementary items to compose an entire outfit, given one extant fashion item and reference masks of target synthesized items. OutfitGAN includes a semantic alignment module, which is responsible for characterizing the mapping correspondence between the existing fashion items and the synthesized ones, to improve the quality of the synthesized images, and a collocation classification module, which is used to improve the compatibility of a synthesized outfit. In order to evaluate the performance of our proposed models, we built a large-scale dataset consisting of 20,000 fashion outfits. Extensive experimental results on this dataset show that our OutfitGAN can synthesize photo-realistic outfits and outperform state-of-the-art methods in terms of similarity, authenticity and compatibility measurements.},
  urldate    = {2025-04-17},
  author     = {Zhou, Dongliang and Zhang, Haijun and Yang, Kai and Liu, Linlin and Yan, Han and Xu, Xiaofei and Zhang, Zhao and Yan, Shuicheng},
  year       = {2025},
  keywords   = {Artificial Intelligence (cs.AI), FOS: Computer and information sciences, Graphics (cs.GR), Machine Learning (cs.LG)}
}

@misc{zhang_learning_2020,
  title     = {Learning {Color} {Compatibility} in {Fashion} {Outfits}},
  copyright = {arXiv.org perpetual, non-exclusive license},
  url       = {https://arxiv.org/abs/2007.02388},
  doi       = {10.48550/ARXIV.2007.02388},
  abstract  = {Color compatibility is important for evaluating the compatibility of a fashion outfit, yet it was neglected in previous studies. We bring this important problem to researchers' attention and present a compatibility learning framework as solution to various fashion tasks. The framework consists of a novel way to model outfit compatibility and an innovative learning scheme. Specifically, we model the outfits as graphs and propose a novel graph construction to better utilize the power of graph neural networks. Then we utilize both ground-truth labels and pseudo labels to train the compatibility model in a weakly-supervised manner.Extensive experimental results verify the importance of color compatibility alone with the effectiveness of our framework. With color information alone, our model's performance is already comparable to previous methods that use deep image features. Our full model combining the aforementioned contributions set the new state-of-the-art in fashion compatibility prediction.},
  urldate   = {2025-04-17},
  publisher = {arXiv},
  author    = {Zhang, Heming and Yang, Xuewen and Tan, Jianchao and Wu, Chi-Hao and Wang, Jue and Kuo, C. -C. Jay},
  year      = {2020},
  keywords  = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences}
}

@article{researcher_revolutionizing_2024,
  title        = {{REVOLUTIONIZING} {FASHION}: {A} {GENERATIVE} {AI} {APPROACH} {TO} {PERSONALIZED} {APPAREL} {DESIGN} {AND} {CUSTOM} {FITTING}},
  copyright    = {Creative Commons Attribution 4.0 International},
  shorttitle   = {{REVOLUTIONIZING} {FASHION}},
  url          = {https://zenodo.org/doi/10.5281/zenodo.13595280},
  doi          = {10.5281/ZENODO.13595280},
  abstract     = {This article presents an innovative approach to personalized apparel design using generative artificial intelligence (AI), addressing the longstanding challenges of fit, style, and accessibility in the fashion industry. The proposed mobile application leverages advanced computer vision, machine learning algorithms, and 3D modeling techniques to offer a fully customizable design experience. The solution bridges the gap between mass-produced and custom-tailored clothing by capturing precise body measurements, simulating fabric textures, and enabling real-time design customization. The core technologies, including 3D body modeling, style and pattern generation through Generative Adversarial Networks (GANs), and real-time rendering, work synergistically to create accurate digital avatars and photorealistic garment visualizations. This AI-driven approach promises enhanced personalization, cost-effective customization, improved sustainability, and increased accessibility in fashion, potentially transforming the industry's design and manufacturing processes.},
  language     = {en},
  urldate      = {2025-04-17},
  author       = {{Researcher}},
  collaborator = {Butteddi, Rajesh Kumar and Butteddi, Srija},
  month        = aug,
  year         = {2024},
  keywords     = {Generative AI, Personalized Fashion, 3D Body Modeling, Virtual Try-on, Sustainable Apparel Design, Computer Vision, GAN}
}

@article{cui_dressing_2019,
  title      = {Dressing as a {Whole}: {Outfit} {Compatibility} {Learning} {Based} on {Node}-wise {Graph} {Neural} {Networks}},
  copyright  = {Creative Commons Zero v1.0 Universal},
  shorttitle = {Dressing as a {Whole}},
  url        = {https://arxiv.org/abs/1902.08009},
  doi        = {10.48550/ARXIV.1902.08009},
  abstract   = {With the rapid development of fashion market, the customers' demands of customers for fashion recommendation are rising. In this paper, we aim to investigate a practical problem of fashion recommendation by answering the question "which item should we select to match with the given fashion items and form a compatible outfit". The key to this problem is to estimate the outfit compatibility. Previous works which focus on the compatibility of two items or represent an outfit as a sequence fail to make full use of the complex relations among items in an outfit. To remedy this, we propose to represent an outfit as a graph. In particular, we construct a Fashion Graph, where each node represents a category and each edge represents interaction between two categories. Accordingly, each outfit can be represented as a subgraph by putting items into their corresponding category nodes. To infer the outfit compatibility from such a graph, we propose Node-wise Graph Neural Networks (NGNN) which can better model node interactions and learn better node representations. In NGNN, the node interaction on each edge is different, which is determined by parameters correlated to the two connected nodes. An attention mechanism is utilized to calculate the outfit compatibility score with learned node representations. NGNN can not only be used to model outfit compatibility from visual or textual modality but also from multiple modalities. We conduct experiments on two tasks: (1) Fill-in-the-blank: suggesting an item that matches with existing components of outfit; (2) Compatibility prediction: predicting the compatibility scores of given outfits. Experimental results demonstrate the great superiority of our proposed method over others.},
  urldate    = {2025-04-17},
  author     = {Cui, Zeyu and Li, Zekun and Wu, Shu and Zhang, Xiaoyu and Wang, Liang},
  year       = {2019},
  keywords   = {FOS: Computer and information sciences, Information Retrieval (cs.IR), Machine Learning (cs.LG)}
}

@misc{moosaei_fashion_2020,
  title     = {Fashion {Recommendation} and {Compatibility} {Prediction} {Using} {Relational} {Network}},
  copyright = {Creative Commons Attribution Non Commercial Share Alike 4.0 International},
  url       = {https://arxiv.org/abs/2005.06584},
  doi       = {10.48550/ARXIV.2005.06584},
  abstract  = {Fashion is an inherently visual concept and computer vision and artificial intelligence (AI) are playing an increasingly important role in shaping the future of this domain. Many research has been done on recommending fashion products based on the learned user preferences. However, in addition to recommending single items, AI can also help users create stylish outfits from items they already have, or purchase additional items that go well with their current wardrobe. Compatibility is the key factor in creating stylish outfits from single items. Previous studies have mostly focused on modeling pair-wise compatibility. There are a few approaches that consider an entire outfit, but these approaches have limitations such as requiring rich semantic information, category labels, and fixed order of items. Thus, they fail to effectively determine compatibility when such information is not available. In this work, we adopt a Relation Network (RN) to develop new compatibility learning models, Fashion RN and FashionRN-VSE, that addresses the limitations of existing approaches. FashionRN learns the compatibility of an entire outfit, with an arbitrary number of items, in an arbitrary order. We evaluated our model using a large dataset of 49,740 outfits that we collected from Polyvore website. Quantitatively, our experimental results demonstrate state of the art performance compared with alternative methods in the literature in both compatibility prediction and fill-in-the-blank test. Qualitatively, we also show that the item embedding learned by FashionRN indicate the compatibility among fashion items.},
  urldate   = {2025-04-17},
  publisher = {arXiv},
  author    = {Moosaei, Maryam and Lin, Yusan and Yang, Hao},
  year      = {2020},
  keywords  = {Artificial Intelligence (cs.AI), FOS: Computer and information sciences, Information Retrieval (cs.IR), Machine Learning (cs.LG)}
}

@inproceedings{xu_diffusion_2024,
  address   = {Washington DC USA},
  title     = {Diffusion {Models} for {Generative} {Outfit} {Recommendation}},
  isbn      = {9798400704314},
  url       = {https://dl.acm.org/doi/10.1145/3626772.3657719},
  doi       = {10.1145/3626772.3657719},
  language  = {en},
  urldate   = {2025-04-17},
  booktitle = {Proceedings of the 47th {International} {ACM} {SIGIR} {Conference} on {Research} and {Development} in {Information} {Retrieval}},
  publisher = {ACM},
  author    = {Xu, Yiyan and Wang, Wenjie and Feng, Fuli and Ma, Yunshan and Zhang, Jizhi and He, Xiangnan},
  month     = jul,
  year      = {2024},
  pages     = {1350--1359}
}

@inproceedings{su_complementary_2021,
  address   = {Virtual Event China},
  title     = {Complementary {Factorization} towards {Outfit} {Compatibility} {Modeling}},
  isbn      = {9781450386517},
  url       = {https://dl.acm.org/doi/10.1145/3474085.3475537},
  doi       = {10.1145/3474085.3475537},
  language  = {en},
  urldate   = {2025-04-17},
  booktitle = {Proceedings of the 29th {ACM} {International} {Conference} on {Multimedia}},
  publisher = {ACM},
  author    = {Su, Tianyu and Song, Xuemeng and Zheng, Na and Guan, Weili and Li, Yan and Nie, Liqiang},
  month     = oct,
  year      = {2021},
  pages     = {4073--4081}
}

@article{chen_survey_2023,
  title     = {A {Survey} of {Artificial} {Intelligence} in {Fashion}},
  volume    = {40},
  copyright = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html},
  issn      = {1053-5888, 1558-0792},
  url       = {https://ieeexplore.ieee.org/document/10113373/},
  doi       = {10.1109/MSP.2022.3233449},
  number    = {3},
  urldate   = {2025-04-17},
  journal   = {IEEE Signal Processing Magazine},
  author    = {Chen, Hung-Jen and Shuai, Hong-Han and Cheng, Wen-Huang},
  month     = may,
  year      = {2023},
  pages     = {64--73}
}

@misc{shih_compatibility_2017,
  title     = {Compatibility {Family} {Learning} for {Item} {Recommendation} and {Generation}},
  copyright = {arXiv.org perpetual, non-exclusive license},
  url       = {https://arxiv.org/abs/1712.01262},
  doi       = {10.48550/ARXIV.1712.01262},
  abstract  = {Compatibility between items, such as clothes and shoes, is a major factor among customer's purchasing decisions. However, learning "compatibility" is challenging due to (1) broader notions of compatibility than those of similarity, (2) the asymmetric nature of compatibility, and (3) only a small set of compatible and incompatible items are observed. We propose an end-to-end trainable system to embed each item into a latent vector and project a query item into K compatible prototypes in the same space. These prototypes reflect the broad notions of compatibility. We refer to both the embedding and prototypes as "Compatibility Family". In our learned space, we introduce a novel Projected Compatibility Distance (PCD) function which is differentiable and ensures diversity by aiming for at least one prototype to be close to a compatible item, whereas none of the prototypes are close to an incompatible item. We evaluate our system on a toy dataset, two Amazon product datasets, and Polyvore outfit dataset. Our method consistently achieves state-of-the-art performance. Finally, we show that we can visualize the candidate compatible prototypes using a Metric-regularized Conditional Generative Adversarial Network (MrCGAN), where the input is a projected prototype and the output is a generated image of a compatible item. We ask human evaluators to judge the relative compatibility between our generated images and images generated by CGANs conditioned directly on query items. Our generated images are significantly preferred, with roughly twice the number of votes as others.},
  urldate   = {2025-04-17},
  publisher = {arXiv},
  author    = {Shih, Yong-Siang and Chang, Kai-Yueh and Lin, Hsuan-Tien and Sun, Min},
  year      = {2017},
  keywords  = {Artificial Intelligence (cs.AI), Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, Machine Learning (cs.LG)}
}

@article{wang_outfit_2019,
  title     = {Outfit {Compatibility} {Prediction} and {Diagnosis} with {Multi}-{Layered} {Comparison} {Network}},
  copyright = {arXiv.org perpetual, non-exclusive license},
  url       = {https://arxiv.org/abs/1907.11496},
  doi       = {10.48550/ARXIV.1907.11496},
  abstract  = {Existing works about fashion outfit compatibility focus on predicting the overall compatibility of a set of fashion items with their information from different modalities. However, there are few works explore how to explain the prediction, which limits the persuasiveness and effectiveness of the model. In this work, we propose an approach to not only predict but also diagnose the outfit compatibility. We introduce an end-to-end framework for this goal, which features for: (1) The overall compatibility is learned from all type-specified pairwise similarities between items, and the backpropagation gradients are used to diagnose the incompatible factors. (2) We leverage the hierarchy of CNN and compare the features at different layers to take into account the compatibilities of different aspects from the low level (such as color, texture) to the high level (such as style). To support the proposed method, we build a new type-specified outfit dataset named Polyvore-T based on Polyvore dataset. We compare our method with the prior state-of-the-art in two tasks: outfit compatibility prediction and fill-in-the-blank. Experiments show that our approach has advantages in both prediction performance and diagnosis ability.},
  urldate   = {2025-04-17},
  author    = {Wang, Xin and Wu, Bo and Ye, Yun and Zhong, Yueqi},
  year      = {2019},
  keywords  = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, Machine Learning (cs.LG), Multimedia (cs.MM)}
}

@inproceedings{zheng_collocation_2021,
  address    = {Virtual Event China},
  title      = {Collocation and {Try}-on {Network}: {Whether} an {Outfit} is {Compatible}},
  isbn       = {9781450386517},
  shorttitle = {Collocation and {Try}-on {Network}},
  url        = {https://dl.acm.org/doi/10.1145/3474085.3475691},
  doi        = {10.1145/3474085.3475691},
  language   = {en},
  urldate    = {2025-04-17},
  booktitle  = {Proceedings of the 29th {ACM} {International} {Conference} on {Multimedia}},
  publisher  = {ACM},
  author     = {Zheng, Na and Song, Xuemeng and Niu, Qingying and Dong, Xue and Zhan, Yibing and Nie, Liqiang},
  month      = oct,
  year       = {2021},
  pages      = {309--317}
}

@misc{li_coherent_2019,
  title     = {Coherent and {Controllable} {Outfit} {Generation}},
  copyright = {Creative Commons Attribution Non Commercial Share Alike 4.0 International},
  url       = {https://arxiv.org/abs/1906.07273},
  doi       = {10.48550/ARXIV.1906.07273},
  abstract  = {When thinking about dressing oneself, people often have a theme in mind whether they're going to a tropical getaway or wish to appear attractive at a cocktail party. A useful outfit generation system should come up with clothing items that are compatible while matching a theme specified by the user. Existing methods use item-wise compatibility between products but lack an effective way to enforce a global constraint (e.g., style, occasion). We introduce a method that generates outfits whose items match a theme described by a text query. Our method uses text and image embeddings to represent fashion items. We learn a multimodal embedding where the image representation for an item is close to its text representation, and use this embedding to measure item-query coherence. We then use a discriminator to compute compatibility between fashion items. This strategy yields a compatibility prediction method that meets or exceeds the state of the art. Our method combines item-item compatibility and item-query coherence to construct an outfit whose items are (a) close to the query and (b) compatible with one another. Quantitative evaluation shows that the items in our outfits are tightly clustered compared to standard outfits. Furthermore, outfits produced by similar queries are close to one another, and outfits produced by very different queries are far apart. Qualitative evaluation shows that our method responds well to queries. A user study suggests that people understand the match between the queries and the outfits produced by our method.},
  urldate   = {2025-04-17},
  publisher = {arXiv},
  author    = {Li, Kedan and Liu, Chen and Forsyth, David},
  year      = {2019},
  keywords  = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences}
}

@article{zhou_coutfitgan_2023,
  title      = {{COutfitGAN}: {Learning} to {Synthesize} {Compatible} {Outfits} {Supervised} by {Silhouette} {Masks} and {Fashion} {Styles}},
  volume     = {25},
  copyright  = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html},
  issn       = {1520-9210, 1941-0077},
  shorttitle = {{COutfitGAN}},
  url        = {https://ieeexplore.ieee.org/document/9804781/},
  doi        = {10.1109/TMM.2022.3185894},
  urldate    = {2025-04-17},
  journal    = {IEEE Transactions on Multimedia},
  author     = {Zhou, Dongliang and Zhang, Haijun and Li, Qun and Ma, Jianghong and Xu, Xiaofei},
  year       = {2023},
  pages      = {4986--5001}
}

@misc{kim_self-supervised_2020,
  title     = {Self-supervised {Visual} {Attribute} {Learning} for {Fashion} {Compatibility}},
  copyright = {arXiv.org perpetual, non-exclusive license},
  url       = {https://arxiv.org/abs/2008.00348},
  doi       = {10.48550/ARXIV.2008.00348},
  abstract  = {Many self-supervised learning (SSL) methods have been successful in learning semantically meaningful visual representations by solving pretext tasks. However, prior work in SSL focuses on tasks like object recognition or detection, which aim to learn object shapes and assume that the features should be invariant to concepts like colors and textures. Thus, these SSL methods perform poorly on downstream tasks where these concepts provide critical information. In this paper, we present an SSL framework that enables us to learn color and texture-aware features without requiring any labels during training. Our approach consists of three self-supervised tasks designed to capture different concepts that are neglected in prior work that we can select from depending on the needs of our downstream tasks. Our tasks include learning to predict color histograms and discriminate shapeless local patches and textures from each instance. We evaluate our approach on fashion compatibility using Polyvore Outfits and In-Shop Clothing Retrieval using Deepfashion, improving upon prior SSL methods by 9.5-16\%, and even outperforming some supervised approaches on Polyvore Outfits despite using no labels. We also show that our approach can be used for transfer learning, demonstrating that we can train on one dataset while achieving high performance on a different dataset.},
  urldate   = {2025-04-17},
  publisher = {arXiv},
  author    = {Kim, Donghyun and Saito, Kuniaki and Mishra, Samarth and Sclaroff, Stan and Saenko, Kate and Plummer, Bryan A},
  year      = {2020},
  keywords  = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences}
}

@article{dong_towards_2025,
  title      = {Towards {Intelligent} {Design}: {A} {Self}-driven {Framework} for {Collocated} {Clothing} {Synthesis} {Leveraging} {Fashion} {Styles} and {Textures}},
  copyright  = {arXiv.org perpetual, non-exclusive license},
  shorttitle = {Towards {Intelligent} {Design}},
  url        = {https://arxiv.org/abs/2501.13396},
  doi        = {10.48550/ARXIV.2501.13396},
  abstract   = {Collocated clothing synthesis (CCS) has emerged as a pivotal topic in fashion technology, primarily concerned with the generation of a clothing item that harmoniously matches a given item. However, previous investigations have relied on using paired outfits, such as a pair of matching upper and lower clothing, to train a generative model for achieving this task. This reliance on the expertise of fashion professionals in the construction of such paired outfits has engendered a laborious and time-intensive process. In this paper, we introduce a new self-driven framework, named style- and texture-guided generative network (ST-Net), to synthesize collocated clothing without the necessity for paired outfits, leveraging self-supervised learning. ST-Net is designed to extrapolate fashion compatibility rules from the style and texture attributes of clothing, using a generative adversarial network. To facilitate the training and evaluation of our model, we have constructed a large-scale dataset specifically tailored for unsupervised CCS. Extensive experiments substantiate that our proposed method outperforms the state-of-the-art baselines in terms of both visual authenticity and fashion compatibility.},
  urldate    = {2025-04-17},
  author     = {Dong, Minglong and Zhou, Dongliang and Ma, Jianghong and Zhang, Haijun},
  year       = {2025},
  keywords   = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences}
}

@article{guo_ai_2023,
  title      = {{AI} {Assisted} {Fashion} {Design}: {A} {Review}},
  volume     = {11},
  copyright  = {https://creativecommons.org/licenses/by-nc-nd/4.0/},
  issn       = {2169-3536},
  shorttitle = {{AI} {Assisted} {Fashion} {Design}},
  url        = {https://ieeexplore.ieee.org/document/10223039/},
  doi        = {10.1109/ACCESS.2023.3306235},
  urldate    = {2025-04-17},
  journal    = {IEEE Access},
  author     = {Guo, Ziyue and Zhu, Zongyang and Li, Yizhi and Cao, Shidong and Chen, Hangyue and Wang, Gaoang},
  year       = {2023},
  pages      = {88403--88415}
}

@article{bhat_review_2025,
  title   = {A {Review} on {Comparative} {Analysis} of {Generative} {Adversarial} {Networks}’ {Architectures} and {Applications}},
  volume  = {6},
  number  = {1},
  journal = {Journal of Robotics and Control (JRC)},
  author  = {Bhat, Ranjith and Nanjundegowda, Raghu},
  year    = {2025},
  pages   = {53--64}
}

@inproceedings{de_vrindt_predicting_2024,
  address   = {Mexico City, Mexico},
  title     = {Predicting {Initial} {Essay} {Quality} {Scores} to {Increase} the {Efficiency} of {Comparative} {Judgment} {Assessments}},
  url       = {https://aclanthology.org/2024.bea-1.12/},
  abstract  = {Comparative judgment (CJ) is a method that can be used to assess the writing quality of student essays based on repeated pairwise comparisons by multiple assessors. Although the assessment method is known to have high validity and reliability, it can be particularly inefficient, as assessors must make many judgments before the scores become reliable. Prior research has investigated methods to improve the efficiency of CJ, yet these methods introduce additional challenges, notably stemming from the initial lack of information at the start of the assessment, which is known as a cold-start problem. This paper reports on a study in which we predict the initial quality scores of essays to establish a warm start for CJ. To achieve this, we construct informative prior distributions for the quality scores based on the predicted initial quality scores. Through simulation studies, we demonstrate that our approach increases the efficiency of CJ: On average, assessors need to make 30\% fewer judgments for each essay to reach an overall reliability level of 0.70.},
  booktitle = {Proceedings of the 19th {Workshop} on {Innovative} {Use} of {NLP} for {Building} {Educational} {Applications} ({BEA} 2024)},
  publisher = {Association for Computational Linguistics},
  author    = {De Vrindt, Michiel and Tack, Anaïs and Bouwer, Renske and Van Den Noortgate, Wim and Lesterhuis, Marije},
  editor    = {Kochmar, Ekaterina and Bexte, Marie and Burstein, Jill and Horbach, Andrea and Laarmann-Quante, Ronja and Tack, Anaïs and Yaneva, Victoria and Yuan, Zheng},
  month     = jun,
  year      = {2024},
  pages     = {125--136}
}

@article{singh_virtual_2024,
  title  = {Virtual {Fashion} {Stylist} with {AI}-{Generated} {Wardrobe} {Capsule} {Suggestions}},
  volume = {11},
  issn   = {2349-5162},
  url    = {http://www.jetir.org/papers/JETIR2411533.pdf},
  number = {11},
  author = {Singh, Mr. T S Bhagavath and P, Srishti K and Taj, Simran and V, Prerana D and {Sinchana U Hegde}},
  month  = nov,
  year   = {2024},
  pages  = {ppf311--f317}
}

@inproceedings{attaluri_styleai_2022,
  title  = {{StyleAI}: {A} {Deep}-{Learning} {Powered} {Outfit} {Generator}},
  url    = {https://api.semanticscholar.org/CorpusID:270493664},
  author = {Attaluri, Nithya},
  year   = {2022}
}

@misc{shi_using_2020,
  title     = {Using {Artificial} {Intelligence} to {Analyze} {Fashion} {Trends}},
  copyright = {arXiv.org perpetual, non-exclusive license},
  url       = {https://arxiv.org/abs/2005.00986},
  doi       = {10.48550/ARXIV.2005.00986},
  abstract  = {Analyzing fashion trends is essential in the fashion industry. Current fashion forecasting firms, such as WGSN, utilize the visual information from around the world to analyze and predict fashion trends. However, analyzing fashion trends is time-consuming and extremely labor intensive, requiring individual employees' manual editing and classification. To improve the efficiency of data analysis of such image-based information and lower the cost of analyzing fashion images, this study proposes a data-driven quantitative abstracting approach using an artificial intelligence (A.I.) algorithm. Specifically, an A.I. model was trained on fashion images from a large-scale dataset under different scenarios, for example in online stores and street snapshots. This model was used to detect garments and classify clothing attributes such as textures, garment style, and details for runway photos and videos. It was found that the A.I. model can generate rich attribute descriptions of detected regions and accurately bind the garments in the images. Adoption of A.I. algorithm demonstrated promising results and the potential to classify garment types and details automatically, which can make the process of trend forecasting more cost-effective and faster.},
  urldate   = {2025-04-17},
  publisher = {arXiv},
  author    = {Shi, Mengyun and Lewis, Van Dyk},
  year      = {2020},
  keywords  = {Artificial Intelligence (cs.AI), Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences}
}

@inproceedings{zheng_virtually_2019,
  address   = {Nice France},
  title     = {Virtually {Trying} on {New} {Clothing} with {Arbitrary} {Poses}},
  isbn      = {9781450368896},
  url       = {https://dl.acm.org/doi/10.1145/3343031.3350946},
  doi       = {10.1145/3343031.3350946},
  language  = {en},
  urldate   = {2025-04-17},
  booktitle = {Proceedings of the 27th {ACM} {International} {Conference} on {Multimedia}},
  publisher = {ACM},
  author    = {Zheng, Na and Song, Xuemeng and Chen, Zhaozheng and Hu, Linmei and Cao, Da and Nie, Liqiang},
  month     = oct,
  year      = {2019},
  pages     = {266--274}
}

@misc{zhao_unifashion_2024,
  title      = {{UniFashion}: {A} {Unified} {Vision}-{Language} {Model} for {Multimodal} {Fashion} {Retrieval} and {Generation}},
  copyright  = {Creative Commons Attribution 4.0 International},
  shorttitle = {{UniFashion}},
  url        = {https://arxiv.org/abs/2408.11305},
  doi        = {10.48550/ARXIV.2408.11305},
  abstract   = {The fashion domain encompasses a variety of real-world multimodal tasks, including multimodal retrieval and multimodal generation. The rapid advancements in artificial intelligence generated content, particularly in technologies like large language models for text generation and diffusion models for visual generation, have sparked widespread research interest in applying these multimodal models in the fashion domain. However, tasks involving embeddings, such as image-to-text or text-to-image retrieval, have been largely overlooked from this perspective due to the diverse nature of the multimodal fashion domain. And current research on multi-task single models lack focus on image generation. In this work, we present UniFashion, a unified framework that simultaneously tackles the challenges of multimodal generation and retrieval tasks within the fashion domain, integrating image generation with retrieval tasks and text generation tasks. UniFashion unifies embedding and generative tasks by integrating a diffusion model and LLM, enabling controllable and high-fidelity generation. Our model significantly outperforms previous single-task state-of-the-art models across diverse fashion tasks, and can be readily adapted to manage complex vision-language tasks. This work demonstrates the potential learning synergy between multimodal generation and retrieval, offering a promising direction for future research in the fashion domain. The source code is available at https://github.com/xiangyu-mm/UniFashion.},
  urldate    = {2025-04-17},
  publisher  = {arXiv},
  author     = {Zhao, Xiangyu and Zhang, Yuehan and Zhang, Wenlong and Wu, Xiao-Ming},
  year       = {2024},
  keywords   = {Artificial Intelligence (cs.AI), Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences}
}

@misc{zhou_learning_2024,
  title     = {Learning {Flow} {Fields} in {Attention} for {Controllable} {Person} {Image} {Generation}},
  copyright = {Creative Commons Attribution 4.0 International},
  url       = {https://arxiv.org/abs/2412.08486},
  doi       = {10.48550/ARXIV.2412.08486},
  abstract  = {Controllable person image generation aims to generate a person image conditioned on reference images, allowing precise control over the person's appearance or pose. However, prior methods often distort fine-grained textural details from the reference image, despite achieving high overall image quality. We attribute these distortions to inadequate attention to corresponding regions in the reference image. To address this, we thereby propose learning flow fields in attention (Leffa), which explicitly guides the target query to attend to the correct reference key in the attention layer during training. Specifically, it is realized via a regularization loss on top of the attention map within a diffusion-based baseline. Our extensive experiments show that Leffa achieves state-of-the-art performance in controlling appearance (virtual try-on) and pose (pose transfer), significantly reducing fine-grained detail distortion while maintaining high image quality. Additionally, we show that our loss is model-agnostic and can be used to improve the performance of other diffusion models.},
  urldate   = {2025-04-17},
  publisher = {arXiv},
  author    = {Zhou, Zijian and Liu, Shikun and Han, Xiao and Liu, Haozhe and Ng, Kam Woh and Xie, Tian and Cong, Yuren and Li, Hang and Xu, Mengmeng and Pérez-Rúa, Juan-Manuel and Patel, Aditya and Xiang, Tao and Shi, Miaojing and He, Sen},
  year      = {2024},
  keywords  = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences}
}

@article{ho_thanh_thuy_building_2024,
  title    = {Building framework recommendation system for trendy fashion e-commerce based on deep learning with {Top}-{K}},
  volume   = {12},
  issn     = {25828185},
  url      = {https://ijsra.net/node/4893},
  doi      = {10.30574/ijsra.2024.12.2.1270},
  abstract = {Recently, e-commerce has become a vital component of our purchasing habits. Central to this evolution is the recommendation system, an advanced algorithm designed to personalize the shopping experience and significantly boost consumer demand. With its diverse and ever-changing inventory, the fashion industry benefits immensely from these algorithms, making it a fascinating case study for understanding the broader impacts of technology on consumerism. Traditional fashion recommendation systems are fundamentally based on item compatibility, but keeping up with trends is also essential. To address this, we propose a two-stage system: fashion detection and outfit suggestions based on the identified items. Users receive images of Key Opinion Leaders (KOLs) or Influencers wearing similar outfits. These recommendations ensure item compatibility, offer diverse styles, and remain fashionable. At the outset, we experimented with YOLOv8 to select the best version. Next, we implemented fashion image retrieval based on feature extraction using two pre-trained networks. To enhance reliability, we developed a voting and ranking algorithm. Our experiments, conducted on a self-collected dataset, evaluated the system’s effectiveness in detecting fashion objects and the efficiency of content-based image retrieval},
  number   = {2},
  urldate  = {2025-04-17},
  journal  = {International Journal of Science and Research Archive},
  author   = {{Ho Thanh Thuy} and {Pham The Bao} and {Do Dieu Le}},
  month    = jul,
  year     = {2024},
  pages    = {664--675}
}

@inproceedings{gerasimova_assessment_2024,
  address   = {Yerevan, Armenia},
  title     = {Assessment of the color compatibility of garments for building a recommendation system for an outfit},
  isbn      = {9781510674622 9781510674639},
  url       = {https://www.spiedigitallibrary.org/conference-proceedings-of-spie/13072/3023213/Assessment-of-the-color-compatibility-of-garments-for-building-a/10.1117/12.3023213.full},
  doi       = {10.1117/12.3023213},
  urldate   = {2025-04-17},
  booktitle = {Sixteenth {International} {Conference} on {Machine} {Vision} ({ICMV} 2023)},
  publisher = {SPIE},
  author    = {Gerasimova, Ekaterina P. and Sholomov, Dmitry L.},
  editor    = {Osten, Wolfgang},
  month     = apr,
  year      = {2024},
  pages     = {8}
}

@article{vo_efficient_2023,
  title    = {An efficient framework for outfit compatibility prediction towards occasion},
  volume   = {35},
  issn     = {0941-0643, 1433-3058},
  url      = {https://link.springer.com/10.1007/s00521-023-08431-1},
  doi      = {10.1007/s00521-023-08431-1},
  abstract = {Abstract 
              Nowadays, in a communicating society, fashion is an integral part of a human life, and it is more comfortable and confident when people dress well. Outfit compatibility is not only a combination of different items but also regarding various aspects, such as style, user preferences, and specific occasions. Most of the existing works lead to address the outfit compatibility concerning only style or user preferences, and have no regard for occasions. In this paper, we propose an efficient method for both outfit compatibility and the fill-in-the-blank tasks according to specific occasions. To this end, we utilized an auxiliary classification branch to learn the significantly important features regarding specific occasions. Besides, a sequence to sequence approach is also applied to learn the relationship of different items along with a visual semantic space, which is able to learn the connection between visual features and their semantic presentation. To demonstrate the effectiveness of the proposed method, we conduct experiments on our newly collected Shoplook-Occasion dataset. The experimental results indicate that our proposed method improved the AUC metric from 0.02 to 0.15\% and from 0.5 to 4\% on accuracy, compared with other approaches for outfit compatibility problem conditioning on specific occasions.},
  language = {en},
  number   = {19},
  urldate  = {2025-04-17},
  journal  = {Neural Computing and Applications},
  author   = {Vo, Anh H. and Le, Tung B. T. and Pham, Huy V. and Nguyen, Bao T.},
  month    = jul,
  year     = {2023},
  pages    = {14213--14226}
}

@misc{deldjoo_review_2022,
  title     = {A {Review} of {Modern} {Fashion} {Recommender} {Systems}},
  copyright = {Creative Commons Attribution 4.0 International},
  url       = {https://arxiv.org/abs/2202.02757},
  doi       = {10.48550/ARXIV.2202.02757},
  abstract  = {The textile and apparel industries have grown tremendously over the last few years. Customers no longer have to visit many stores, stand in long queues, or try on garments in dressing rooms as millions of products are now available in online catalogs. However, given the plethora of options available, an effective recommendation system is necessary to properly sort, order, and communicate relevant product material or information to users. Effective fashion RS can have a noticeable impact on billions of customers' shopping experiences and increase sales and revenues on the provider side. The goal of this survey is to provide a review of recommender systems that operate in the specific vertical domain of garment and fashion products. We have identified the most pressing challenges in fashion RS research and created a taxonomy that categorizes the literature according to the objective they are trying to accomplish (e.g., item or outfit recommendation, size recommendation, explainability, among others) and type of side-information (users, items, context). We have also identified the most important evaluation goals and perspectives (outfit generation, outfit recommendation, pairing recommendation, and fill-in-the-blank outfit compatibility prediction) and the most commonly used datasets and evaluation metrics.},
  urldate   = {2025-04-17},
  publisher = {arXiv},
  author    = {Deldjoo, Yashar and Nazary, Fatemeh and Ramisa, Arnau and Mcauley, Julian and Pellegrini, Giovanni and Bellogin, Alejandro and Di Noia, Tommaso},
  year      = {2022},
  keywords  = {FOS: Computer and information sciences, Information Retrieval (cs.IR)}
}

@misc{cheng_fashion_2020,
  title      = {Fashion {Meets} {Computer} {Vision}: {A} {Survey}},
  copyright  = {arXiv.org perpetual, non-exclusive license},
  shorttitle = {Fashion {Meets} {Computer} {Vision}},
  url        = {https://arxiv.org/abs/2003.13988},
  doi        = {10.48550/ARXIV.2003.13988},
  abstract   = {Fashion is the way we present ourselves to the world and has become one of the world's largest industries. Fashion, mainly conveyed by vision, has thus attracted much attention from computer vision researchers in recent years. Given the rapid development, this paper provides a comprehensive survey of more than 200 major fashion-related works covering four main aspects for enabling intelligent fashion: (1) Fashion detection includes landmark detection, fashion parsing, and item retrieval, (2) Fashion analysis contains attribute recognition, style learning, and popularity prediction, (3) Fashion synthesis involves style transfer, pose transformation, and physical simulation, and (4) Fashion recommendation comprises fashion compatibility, outfit matching, and hairstyle suggestion. For each task, the benchmark datasets and the evaluation protocols are summarized. Furthermore, we highlight promising directions for future research.},
  urldate    = {2025-04-17},
  publisher  = {arXiv},
  author     = {Cheng, Wen-Huang and Song, Sijie and Chen, Chieh-Yun and Hidayati, Shintami Chusnul and Liu, Jiaying},
  year       = {2020},
  keywords   = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences}
}

@inproceedings{kang_visually-aware_2017,
  address   = {New Orleans, LA},
  title     = {Visually-{Aware} {Fashion} {Recommendation} and {Design} with {Generative} {Image} {Models}},
  isbn      = {9781538638354},
  url       = {http://ieeexplore.ieee.org/document/8215493/},
  doi       = {10.1109/ICDM.2017.30},
  urldate   = {2025-04-16},
  booktitle = {2017 {IEEE} {International} {Conference} on {Data} {Mining} ({ICDM})},
  publisher = {IEEE},
  author    = {Kang, Wang-Cheng and Fang, Chen and Wang, Zhaowen and McAuley, Julian},
  month     = nov,
  year      = {2017},
  pages     = {207--216}
}

@misc{zheng_deep_2020,
  title      = {Deep {Learning}-{Based} {Human} {Pose} {Estimation}: {A} {Survey}},
  copyright  = {arXiv.org perpetual, non-exclusive license},
  shorttitle = {Deep {Learning}-{Based} {Human} {Pose} {Estimation}},
  url        = {https://arxiv.org/abs/2012.13392},
  doi        = {10.48550/ARXIV.2012.13392},
  abstract   = {Human pose estimation aims to locate the human body parts and build human body representation (e.g., body skeleton) from input data such as images and videos. It has drawn increasing attention during the past decade and has been utilized in a wide range of applications including human-computer interaction, motion analysis, augmented reality, and virtual reality. Although the recently developed deep learning-based solutions have achieved high performance in human pose estimation, there still remain challenges due to insufficient training data, depth ambiguities, and occlusion. The goal of this survey paper is to provide a comprehensive review of recent deep learning-based solutions for both 2D and 3D pose estimation via a systematic analysis and comparison of these solutions based on their input data and inference procedures. More than 250 research papers since 2014 are covered in this survey. Furthermore, 2D and 3D human pose estimation datasets and evaluation metrics are included. Quantitative performance comparisons of the reviewed methods on popular datasets are summarized and discussed. Finally, the challenges involved, applications, and future research directions are concluded. A regularly updated project page is provided: {\textbackslash}url\{https://github.com/zczcwh/DL-HPE\}},
  urldate    = {2025-04-16},
  publisher  = {arXiv},
  author     = {Zheng, Ce and Wu, Wenhan and Chen, Chen and Yang, Taojiannan and Zhu, Sijie and Shen, Ju and Kehtarnavaz, Nasser and Shah, Mubarak},
  year       = {2020},
  keywords   = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, Graphics (cs.GR), Multimedia (cs.MM)}
}

@article{zhang_unlocking_2024,
  title      = {Unlocking the {Potential} of {Artificial} {Intelligence} in {Fashion} {Design} and {E}-{Commerce} {Applications}: {The} {Case} of {Midjourney}},
  volume     = {19},
  copyright  = {https://creativecommons.org/licenses/by/4.0/},
  issn       = {0718-1876},
  shorttitle = {Unlocking the {Potential} of {Artificial} {Intelligence} in {Fashion} {Design} and {E}-{Commerce} {Applications}},
  url        = {https://www.mdpi.com/0718-1876/19/1/35},
  doi        = {10.3390/jtaer19010035},
  abstract   = {The fashion industry has shown increasing interest in applying artificial intelligence (AI), yet there is a significant gap in exploring the potential of emerging diffusion-modeling-based AI image-generation systems for fashion design and commerce. Therefore, this study aims to assess the effectiveness of Midjourney, one such AI system, in both fashion design and related commerce applications. We employed the action research approach with the Functional, Expressive, and Aesthetic (FEA) Consumer Needs Model as the theoretical framework. Our research comprised three stages: refining an initial idea into well-defined textual design concepts, facilitating concept development, and validating the preceding observations and reflections by creating a new line of hemp-based products that were evaluated by targeted consumers through an online survey. Findings reveal that this AI tool can assist fashion designers in creating both visually expressive attire and ready-to-wear products, meeting defined design criteria and consumer needs. Midjourney shows promise in streamlining the fashion design process by enhancing ideation and optimizing design details. Potential e-commercial applications of such AI systems were proposed, benefiting physical and digital fashion businesses. It is noted that, to date, the major limitations of using Midjourney encompass its restriction to only facilitating early fashion design stages and necessitating substantial involvement from designers.},
  language   = {en},
  number     = {1},
  urldate    = {2025-04-16},
  journal    = {Journal of Theoretical and Applied Electronic Commerce Research},
  author     = {Zhang, Yanbo and Liu, Chuanlan},
  month      = mar,
  year       = {2024},
  pages      = {654--670}
}

@article{shirkhani_study_2023,
  title    = {Study of {AI}-{Driven} {Fashion} {Recommender} {Systems}},
  volume   = {4},
  issn     = {2661-8907},
  url      = {https://link.springer.com/10.1007/s42979-023-01932-9},
  doi      = {10.1007/s42979-023-01932-9},
  abstract = {Abstract 
              The rising diversity, volume, and pace of fashion manufacturing pose a considerable challenge in the fashion industry, making it difficult for customers to pick which product to purchase. In addition, fashion is an inherently subjective, cultural notion and an ensemble of clothing items that maintains a coherent style. In most of the domains in which Recommender Systems are developed (e.g., movies, e-commerce, etc.), the similarity evaluation is considered for recommendation. Instead, in the Fashion domain, compatibility is a critical factor. In addition, raw visual features belonging to product representations that contribute to most of the algorithm’s performances in the Fashion domain are distinguishable from the metadata of the products in other domains. This literature review summarizes various Artificial Intelligence (AI) techniques that have lately been used in recommender systems for the fashion industry. AI enables higher-quality recommendations than earlier approaches. This has ushered in a new age for recommender systems, allowing for deeper insights into user-item relationships and representations and the discovery patterns in demographical, textual, virtual, and contextual data. This work seeks to give a deeper understanding of the fashion recommender system domain by performing a comprehensive literature study of research on this topic in the past 10 years, focusing on image-based fashion recommender systems taking AI improvements into account. The nuanced conceptions of this domain and their relevance have been developed to justify fashion domain-specific characteristics.},
  language = {en},
  number   = {5},
  urldate  = {2025-04-16},
  journal  = {SN Computer Science},
  author   = {Shirkhani, Shaghayegh and Mokayed, Hamam and Saini, Rajkumar and Chai, Hum Yan},
  month    = jul,
  year     = {2023},
  pages    = {514}
}

@misc{kimura_shift15m_2021,
  title      = {{SHIFT15M}: {Fashion}-specific dataset for set-to-set matching with several distribution shifts},
  copyright  = {Creative Commons Attribution 4.0 International},
  shorttitle = {{SHIFT15M}},
  url        = {https://arxiv.org/abs/2108.12992},
  doi        = {10.48550/ARXIV.2108.12992},
  abstract   = {This paper addresses the problem of set-to-set matching, which involves matching two different sets of items based on some criteria, especially in the case of high-dimensional items like images. Although neural networks have been applied to solve this problem, most machine learning-based approaches assume that the training and test data follow the same distribution, which is not always true in real-world scenarios. To address this limitation, we introduce SHIFT15M, a dataset that can be used to evaluate set-to-set matching models when the distribution of data changes between training and testing. We conduct benchmark experiments that demonstrate the performance drop of naive methods due to distribution shift. Additionally, we provide software to handle the SHIFT15M dataset in a simple manner, with the URL for the software to be made available after publication of this manuscript. We believe proposed SHIFT15M dataset provide a valuable resource for evaluating set-to-set matching models under the distribution shift.},
  urldate    = {2025-04-16},
  publisher  = {arXiv},
  author     = {Kimura, Masanari and Nakamura, Takuma and Saito, Yuki},
  year       = {2021},
  keywords   = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, Machine Learning (cs.LG)}
}

@inproceedings{chen_pog_2019,
  address    = {Anchorage AK USA},
  title      = {{POG}: {Personalized} {Outfit} {Generation} for {Fashion} {Recommendation} at {Alibaba} {iFashion}},
  isbn       = {9781450362016},
  shorttitle = {{POG}},
  url        = {https://dl.acm.org/doi/10.1145/3292500.3330652},
  doi        = {10.1145/3292500.3330652},
  language   = {en},
  urldate    = {2025-04-16},
  booktitle  = {Proceedings of the 25th {ACM} {SIGKDD} {International} {Conference} on {Knowledge} {Discovery} \& {Data} {Mining}},
  publisher  = {ACM},
  author     = {Chen, Wen and Huang, Pipei and Xu, Jiaming and Guo, Xin and Guo, Cheng and Sun, Fei and Li, Chao and Pfadler, Andreas and Zhao, Huan and Zhao, Binqiang},
  month      = jul,
  year       = {2019},
  pages      = {2662--2670}
}

@misc{sarkar_outfittransformer_2022,
  title      = {{OutfitTransformer}: {Learning} {Outfit} {Representations} for {Fashion} {Recommendation}},
  copyright  = {Creative Commons Attribution 4.0 International},
  shorttitle = {{OutfitTransformer}},
  url        = {https://arxiv.org/abs/2204.04812},
  doi        = {10.48550/ARXIV.2204.04812},
  abstract   = {Learning an effective outfit-level representation is critical for predicting the compatibility of items in an outfit, and retrieving complementary items for a partial outfit. We present a framework, OutfitTransformer, that uses the proposed task-specific tokens and leverages the self-attention mechanism to learn effective outfit-level representations encoding the compatibility relationships between all items in the entire outfit for addressing both compatibility prediction and complementary item retrieval tasks. For compatibility prediction, we design an outfit token to capture a global outfit representation and train the framework using a classification loss. For complementary item retrieval, we design a target item token that additionally takes the target item specification (in the form of a category or text description) into consideration. We train our framework using a proposed set-wise outfit ranking loss to generate a target item embedding given an outfit, and a target item specification as inputs. The generated target item embedding is then used to retrieve compatible items that match the rest of the outfit. Additionally, we adopt a pre-training approach and a curriculum learning strategy to improve retrieval performance. Since our framework learns at an outfit-level, it allows us to learn a single embedding capturing higher-order relations among multiple items in the outfit more effectively than pairwise methods. Experiments demonstrate that our approach outperforms state-of-the-art methods on compatibility prediction, fill-in-the-blank, and complementary item retrieval tasks. We further validate the quality of our retrieval results with a user study.},
  urldate    = {2025-04-16},
  publisher  = {arXiv},
  author     = {Sarkar, Rohan and Bodla, Navaneeth and Vasileva, Mariya I. and Lin, Yen-Liang and Beniwal, Anurag and Lu, Alan and Medioni, Gerard},
  year       = {2022},
  keywords   = {Artificial Intelligence (cs.AI), Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, Information Retrieval (cs.IR), Machine Learning (cs.LG)}
}

@inproceedings{moosaei_outfitgan_2022,
  address    = {New Orleans, LA, USA},
  title      = {{OutfitGAN}: {Learning} {Compatible} {Items} for {Generative} {Fashion} {Outfits}},
  copyright  = {https://doi.org/10.15223/policy-029},
  isbn       = {9781665487399},
  shorttitle = {{OutfitGAN}},
  url        = {https://ieeexplore.ieee.org/document/9857247/},
  doi        = {10.1109/CVPRW56347.2022.00251},
  urldate    = {2025-04-16},
  booktitle  = {2022 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} {Workshops} ({CVPRW})},
  publisher  = {IEEE},
  author     = {Moosaei, Maryam and Lin, Yusan and Akhazhanov, Ablaikhan and Chen, Huiyuan and Wang, Fei and Yang, Hao},
  month      = jun,
  year       = {2022},
  pages      = {2272--2276}
}

@article{garrido_duality_2022,
  title     = {On the duality between contrastive and non-contrastive self-supervised learning},
  copyright = {arXiv.org perpetual, non-exclusive license},
  url       = {https://arxiv.org/abs/2206.02574},
  doi       = {10.48550/ARXIV.2206.02574},
  abstract  = {Recent approaches in self-supervised learning of image representations can be categorized into different families of methods and, in particular, can be divided into contrastive and non-contrastive approaches. While differences between the two families have been thoroughly discussed to motivate new approaches, we focus more on the theoretical similarities between them. By designing contrastive and covariance based non-contrastive criteria that can be related algebraically and shown to be equivalent under limited assumptions, we show how close those families can be. We further study popular methods and introduce variations of them, allowing us to relate this theoretical result to current practices and show the influence (or lack thereof) of design choices on downstream performance. Motivated by our equivalence result, we investigate the low performance of SimCLR and show how it can match VICReg's with careful hyperparameter tuning, improving significantly over known baselines. We also challenge the popular assumption that non-contrastive methods need large output dimensions. Our theoretical and quantitative results suggest that the numerical gaps between contrastive and non-contrastive methods in certain regimes can be closed given better network design choices and hyperparameter tuning. The evidence shows that unifying different SOTA methods is an important direction to build a better understanding of self-supervised learning.},
  urldate   = {2025-04-16},
  author    = {Garrido, Quentin and Chen, Yubei and Bardes, Adrien and Najman, Laurent and Lecun, Yann},
  year      = {2022},
  keywords  = {Artificial Intelligence (cs.AI), Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, Machine Learning (cs.LG)}
}

@misc{sarkar_neural_2021,
  title     = {Neural {Re}-{Rendering} of {Humans} from a {Single} {Image}},
  copyright = {arXiv.org perpetual, non-exclusive license},
  url       = {https://arxiv.org/abs/2101.04104},
  doi       = {10.48550/ARXIV.2101.04104},
  abstract  = {Human re-rendering from a single image is a starkly under-constrained problem, and state-of-the-art algorithms often exhibit undesired artefacts, such as over-smoothing, unrealistic distortions of the body parts and garments, or implausible changes of the texture. To address these challenges, we propose a new method for neural re-rendering of a human under a novel user-defined pose and viewpoint, given one input image. Our algorithm represents body pose and shape as a parametric mesh which can be reconstructed from a single image and easily reposed. Instead of a colour-based UV texture map, our approach further employs a learned high-dimensional UV feature map to encode appearance. This rich implicit representation captures detailed appearance variation across poses, viewpoints, person identities and clothing styles better than learned colour texture maps. The body model with the rendered feature maps is fed through a neural image-translation network that creates the final rendered colour image. The above components are combined in an end-to-end-trained neural network architecture that takes as input a source person image, and images of the parametric body model in the source pose and desired target pose. Experimental evaluation demonstrates that our approach produces higher quality single image re-rendering results than existing methods.},
  urldate   = {2025-04-16},
  publisher = {arXiv},
  author    = {Sarkar, Kripasindhu and Mehta, Dushyant and Xu, Weipeng and Golyanik, Vladislav and Theobalt, Christian},
  year      = {2021},
  keywords  = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences}
}

@article{li_mining_2016,
  title     = {Mining {Fashion} {Outfit} {Composition} {Using} {An} {End}-to-{End} {Deep} {Learning} {Approach} on {Set} {Data}},
  copyright = {arXiv.org perpetual, non-exclusive license},
  url       = {https://arxiv.org/abs/1608.03016},
  doi       = {10.48550/ARXIV.1608.03016},
  abstract  = {Composing fashion outfits involves deep understanding of fashion standards while incorporating creativity for choosing multiple fashion items (e.g., Jewelry, Bag, Pants, Dress). In fashion websites, popular or high-quality fashion outfits are usually designed by fashion experts and followed by large audiences. In this paper, we propose a machine learning system to compose fashion outfits automatically. The core of the proposed automatic composition system is to score fashion outfit candidates based on the appearances and meta-data. We propose to leverage outfit popularity on fashion oriented websites to supervise the scoring component. The scoring component is a multi-modal multi-instance deep learning system that evaluates instance aesthetics and set compatibility simultaneously. In order to train and evaluate the proposed composition system, we have collected a large scale fashion outfit dataset with 195K outfits and 368K fashion items from Polyvore. Although the fashion outfit scoring and composition is rather challenging, we have achieved an AUC of 85\% for the scoring component, and an accuracy of 77\% for a constrained composition task.},
  urldate   = {2025-04-16},
  author    = {Li, Yuncheng and Cao, LiangLiang and Zhu, Jiang and Luo, Jiebo},
  year      = {2016},
  keywords  = {FOS: Computer and information sciences, Machine Learning (cs.LG), Multimedia (cs.MM)}
}

@article{liu_mgcm_2020,
  title      = {{MGCM}: {Multi}-modal generative compatibility modeling for clothing matching},
  volume     = {414},
  issn       = {09252312},
  shorttitle = {{MGCM}},
  url        = {https://linkinghub.elsevier.com/retrieve/pii/S0925231220310043},
  doi        = {10.1016/j.neucom.2020.06.033},
  language   = {en},
  urldate    = {2025-04-16},
  journal    = {Neurocomputing},
  author     = {Liu, Jinhuan and Song, Xuemeng and Chen, Zhumin and Ma, Jun},
  month      = nov,
  year       = {2020},
  pages      = {215--224}
}

@misc{vasileva_learning_2018,
  title     = {Learning {Type}-{Aware} {Embeddings} for {Fashion} {Compatibility}},
  copyright = {arXiv.org perpetual, non-exclusive license},
  url       = {https://arxiv.org/abs/1803.09196},
  doi       = {10.48550/ARXIV.1803.09196},
  abstract  = {Outfits in online fashion data are composed of items of many different types (e.g. top, bottom, shoes) that share some stylistic relationship with one another. A representation for building outfits requires a method that can learn both notions of similarity (for example, when two tops are interchangeable) and compatibility (items of possibly different type that can go together in an outfit). This paper presents an approach to learning an image embedding that respects item type, and jointly learns notions of item similarity and compatibility in an end-to-end model. To evaluate the learned representation, we crawled 68,306 outfits created by users on the Polyvore website. Our approach obtains 3-5\% improvement over the state-of-the-art on outfit compatibility prediction and fill-in-the-blank tasks using our dataset, as well as an established smaller dataset, while supporting a variety of useful queries.},
  urldate   = {2025-04-16},
  publisher = {arXiv},
  author    = {Vasileva, Mariya I. and Plummer, Bryan A. and Dusad, Krishna and Rajpal, Shreya and Kumar, Ranjitha and Forsyth, David},
  year      = {2018},
  keywords  = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences}
}

@misc{javaji_hybrid_2023,
  title     = {Hybrid {Recommendation} {System} using {Graph} {Neural} {Network} and {BERT} {Embeddings}},
  copyright = {Creative Commons Attribution Share Alike 4.0 International},
  url       = {https://arxiv.org/abs/2310.04878},
  doi       = {10.48550/ARXIV.2310.04878},
  abstract  = {Recommender systems have emerged as a crucial component of the modern web ecosystem. The effectiveness and accuracy of such systems are critical for providing users with personalized recommendations that meet their specific interests and needs. In this paper, we introduce a novel model that utilizes a Graph Neural Network (GNN) in conjunction with sentence transformer embeddings to predict anime recommendations for different users. Our model employs the task of link prediction to create a recommendation system that considers both the features of anime and user interactions with different anime. The hybridization of the GNN and transformer embeddings enables us to capture both inter-level and intra-level features of anime data.Our model not only recommends anime to users but also predicts the rating a specific user would give to an anime. We utilize the GraphSAGE network for model building and weighted root mean square error (RMSE) to evaluate the performance of the model. Our approach has the potential to significantly enhance the accuracy and effectiveness of anime recommendation systems and can be extended to other domains that require personalized recommendations.},
  urldate   = {2025-04-16},
  publisher = {arXiv},
  author    = {Javaji, Shashidhar Reddy and Sarode, Krutika},
  year      = {2023},
  keywords  = {Artificial Intelligence (cs.AI), FOS: Computer and information sciences, Information Retrieval (cs.IR), Machine Learning (cs.LG)}
}

@misc{kalinin_generative_2024,
  title     = {Generative {AI}-based {Style} {Recommendation} {Using} {Fashion} {ItemDetection} and {Classification}},
  copyright = {https://creativecommons.org/licenses/by/4.0/},
  url       = {https://www.researchsquare.com/article/rs-4517638/v1},
  doi       = {10.21203/rs.3.rs-4517638/v1},
  abstract  = {Abstract 
               This research work focuses on the creation of a cutting-edge style recommendation system that uses generative AI and deep learning approaches to analyse fashion photos. The system is intended to process input images, such as selfies or studio-quality photos, and output a text file with extensive feedback on the individual's style and suggestions for improvement. The system consists of two main components: the YOLOv8 convolutional neural network, which detects and crops clothing items, and the GPT-4.0 large language model, which generates informative style commentary and recommendations. YOLOv8 is briefly trained on a specific dataset to improve its performance in recognising 10 different types of clothes, while GPT-4.0, which is accessible via the OpenAI API, is charged with giving cohesive and short style suggestions. To evaluate the success of the suggested solution, real experimental trials were conducted at many events in Madrid and Tallinn. Three well-known AI models were used for comparison: OpenAI's GPT-4.0 Vision, Google's Gemini 1.5 Pro, and Anthropic's Claude 3  - Opus. Participants judged the quality of each model's fashion recommendations. The results showed that GPT-4.0 Vision and Gemini 1.5 Pro had comparable average ratings, indicating higher perceived quality than Claude 3 - Opus. This research work demonstrates how cutting-edge computer vision and natural language processing technology may transform personalised fashion advising services, improving accuracy and relevance of style recommendations.},
  urldate   = {2025-04-16},
  author    = {Kalinin, Aleksandr and Jafari, Akbar Anbar and Avots, Egils and Ozcinar, Cagri and Anbarjafari, Gholamreza},
  month     = jun,
  year      = {2024}
}

@article{zhou_fcboost-net_2025,
  title      = {{FCBoost}-{Net}: {A} {Generative} {Network} for {Synthesizing} {Multiple} {Collocated} {Outfits} via {Fashion} {Compatibility} {Boosting}},
  copyright  = {arXiv.org perpetual, non-exclusive license},
  shorttitle = {{FCBoost}-{Net}},
  url        = {https://arxiv.org/abs/2502.00992},
  doi        = {10.48550/ARXIV.2502.00992},
  abstract   = {Outfit generation is a challenging task in the field of fashion technology, in which the aim is to create a collocated set of fashion items that complement a given set of items. Previous studies in this area have been limited to generating a unique set of fashion items based on a given set of items, without providing additional options to users. This lack of a diverse range of choices necessitates the development of a more versatile framework. However, when the task of generating collocated and diversified outfits is approached with multimodal image-to-image translation methods, it poses a challenging problem in terms of non-aligned image translation, which is hard to address with existing methods. In this research, we present FCBoost-Net, a new framework for outfit generation that leverages the power of pre-trained generative models to produce multiple collocated and diversified outfits. Initially, FCBoost-Net randomly synthesizes multiple sets of fashion items, and the compatibility of the synthesized sets is then improved in several rounds using a novel fashion compatibility booster. This approach was inspired by boosting algorithms and allows the performance to be gradually improved in multiple steps. Empirical evidence indicates that the proposed strategy can improve the fashion compatibility of randomly synthesized fashion items as well as maintain their diversity. Extensive experiments confirm the effectiveness of our proposed framework with respect to visual authenticity, diversity, and fashion compatibility.},
  urldate    = {2025-04-16},
  author     = {Zhou, Dongliang and Zhang, Haijun and Ma, Jianghong and Fan, Jicong and Zhang, Zhao},
  year       = {2025},
  keywords   = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, Multimedia (cs.MM)}
}

@misc{gulati_fashion_2024,
  title      = {Fashion {Recommendation}: {Outfit} {Compatibility} using {GNN}},
  copyright  = {Creative Commons Attribution 4.0 International},
  shorttitle = {Fashion {Recommendation}},
  url        = {https://arxiv.org/abs/2404.18040},
  doi        = {10.48550/ARXIV.2404.18040},
  abstract   = {Numerous industries have benefited from the use of machine learning and fashion in industry is no exception. By gaining a better understanding of what makes a good outfit, companies can provide useful product recommendations to their users. In this project, we follow two existing approaches that employ graphs to represent outfits and use modified versions of the Graph neural network (GNN) frameworks. Both Node-wise Graph Neural Network (NGNN) and Hypergraph Neural Network aim to score a set of items according to the outfit compatibility of items. The data used is the Polyvore Dataset which consists of curated outfits with product images and text descriptions for each product in an outfit. We recreate the analysis on a subset of this data and compare the two existing models on their performance on two tasks Fill in the blank (FITB): finding an item that completes an outfit, and Compatibility prediction: estimating compatibility of different items grouped as an outfit. We can replicate the results directionally and find that HGNN does have a slightly better performance on both tasks. On top of replicating the results of the two papers we also tried to use embeddings generated from a vision transformer and witness enhanced prediction accuracy across the board},
  urldate    = {2025-04-16},
  publisher  = {arXiv},
  author     = {Gulati, Samaksh},
  year       = {2024},
  keywords   = {Computation and Language (cs.CL), FOS: Computer and information sciences}
}

@misc{chia_does_2022,
  title      = {"{Does} it come in black?" {CLIP}-like models are zero-shot recommenders},
  copyright  = {arXiv.org perpetual, non-exclusive license},
  shorttitle = {"{Does} it come in black?},
  url        = {https://arxiv.org/abs/2204.02473},
  doi        = {10.48550/ARXIV.2204.02473},
  abstract   = {Product discovery is a crucial component for online shopping. However, item-to-item recommendations today do not allow users to explore changes along selected dimensions: given a query item, can a model suggest something similar but in a different color? We consider item recommendations of the comparative nature (e.g. "something darker") and show how CLIP-based models can support this use case in a zero-shot manner. Leveraging a large model built for fashion, we introduce GradREC and its industry potential, and offer a first rounded assessment of its strength and weaknesses.},
  urldate    = {2025-04-16},
  publisher  = {arXiv},
  author     = {Chia, Patrick John and Tagliabue, Jacopo and Bianchi, Federico and Greco, Ciro and Goncalves, Diogo},
  year       = {2022},
  keywords   = {Artificial Intelligence (cs.AI), FOS: Computer and information sciences, Information Retrieval (cs.IR)}
}

@article{kammoun_generative_2022,
  title      = {Generative {Adversarial} {Networks} for face generation: {A} survey},
  issn       = {0360-0300, 1557-7341},
  shorttitle = {Generative {Adversarial} {Networks} for face generation},
  url        = {https://dl.acm.org/doi/10.1145/1122445.1122456},
  doi        = {10.1145/1122445.1122456},
  abstract   = {Recently, Generative Adversarial Networks (GANs) have received enormous progress, which makes them able to learn complex data distributions in particular faces. More and more efficient GAN architectures have been designed and proposed to learn the different variations of faces, such as cross pose, age, expression and style. These GAN based approaches need to be reviewed, discussed, and categorized in terms of architectures, applications, and metrics. Several reviews that focus on the use and advances of GAN in general have been proposed. However, the GAN models applied to the face, that we call facial GANs, have never been addressed. In this article, we review facial GANs and their different applications. We mainly focus on architectures, problems and performance evaluation with respect to each application and used datasets. More precisely, we reviewed the progress of architectures and we discussed the contributions and limits of each. Then, we exposed the encountered problems of facial GANs and proposed solutions to handle them. Additionally, as GANs evaluation has become a notable current defiance, we investigate the state of the art quantitative and qualitative evaluation metrics and their applications. We concluded the article with a discussion on the face generation challenges and proposed open research issues.},
  language   = {en},
  urldate    = {2025-04-16},
  journal    = {ACM Computing Surveys},
  author     = {Kammoun, Amina and Slama, Rim and Tabia, Hedi and Ouni, Tarek and Abid, Mohmed},
  month      = mar,
  year       = {2022},
  pages      = {1122445.1122456}
}

@misc{forouzandehmehr_decoding_2024,
  title      = {Decoding {Style}: {Efficient} {Fine}-{Tuning} of {LLMs} for {Image}-{Guided} {Outfit} {Recommendation} with {Preference}},
  copyright  = {arXiv.org perpetual, non-exclusive license},
  shorttitle = {Decoding {Style}},
  url        = {https://arxiv.org/abs/2409.12150},
  doi        = {10.48550/ARXIV.2409.12150},
  abstract   = {Personalized outfit recommendation remains a complex challenge, demanding both fashion compatibility understanding and trend awareness. This paper presents a novel framework that harnesses the expressive power of large language models (LLMs) for this task, mitigating their "black box" and static nature through fine-tuning and direct feedback integration. We bridge the item visual-textual gap in items descriptions by employing image captioning with a Multimodal Large Language Model (MLLM). This enables the LLM to extract style and color characteristics from human-curated fashion images, forming the basis for personalized recommendations. The LLM is efficiently fine-tuned on the open-source Polyvore dataset of curated fashion images, optimizing its ability to recommend stylish outfits. A direct preference mechanism using negative examples is employed to enhance the LLM's decision-making process. This creates a self-enhancing AI feedback loop that continuously refines recommendations in line with seasonal fashion trends. Our framework is evaluated on the Polyvore dataset, demonstrating its effectiveness in two key tasks: fill-in-the-blank, and complementary item retrieval. These evaluations underline the framework's ability to generate stylish, trend-aligned outfit suggestions, continuously improving through direct feedback. The evaluation results demonstrated that our proposed framework significantly outperforms the base LLM, creating more cohesive outfits. The improved performance in these tasks underscores the proposed framework's potential to enhance the shopping experience with accurate suggestions, proving its effectiveness over the vanilla LLM based outfit generation.},
  urldate    = {2025-04-16},
  publisher  = {arXiv},
  author     = {Forouzandehmehr, Najmeh and Farrokhsiar, Nima and Giahi, Ramin and Korpeoglu, Evren and Achan, Kannan},
  year       = {2024},
  keywords   = {Artificial Intelligence (cs.AI), FOS: Computer and information sciences, Information Retrieval (cs.IR), Machine Learning (cs.LG)}
}

@misc{balestriero_contrastive_2022,
  title     = {Contrastive and {Non}-{Contrastive} {Self}-{Supervised} {Learning} {Recover} {Global} and {Local} {Spectral} {Embedding} {Methods}},
  copyright = {Creative Commons Attribution 4.0 International},
  url       = {https://arxiv.org/abs/2205.11508},
  doi       = {10.48550/ARXIV.2205.11508},
  abstract  = {Self-Supervised Learning (SSL) surmises that inputs and pairwise positive relationships are enough to learn meaningful representations. Although SSL has recently reached a milestone: outperforming supervised methods in many modalities{\textbackslash}dots the theoretical foundations are limited, method-specific, and fail to provide principled design guidelines to practitioners. In this paper, we propose a unifying framework under the helm of spectral manifold learning to address those limitations. Through the course of this study, we will rigorously demonstrate that VICReg, SimCLR, BarlowTwins et al. correspond to eponymous spectral methods such as Laplacian Eigenmaps, Multidimensional Scaling et al. This unification will then allow us to obtain (i) the closed-form optimal representation for each method, (ii) the closed-form optimal network parameters in the linear regime for each method, (iii) the impact of the pairwise relations used during training on each of those quantities and on downstream task performances, and most importantly, (iv) the first theoretical bridge between contrastive and non-contrastive methods towards global and local spectral embedding methods respectively, hinting at the benefits and limitations of each. For example, (i) if the pairwise relation is aligned with the downstream task, any SSL method can be employed successfully and will recover the supervised method, but in the low data regime, VICReg's invariance hyper-parameter should be high; (ii) if the pairwise relation is misaligned with the downstream task, VICReg with small invariance hyper-parameter should be preferred over SimCLR or BarlowTwins.},
  urldate   = {2025-04-16},
  publisher = {arXiv},
  author    = {Balestriero, Randall and LeCun, Yann},
  year      = {2022},
  keywords  = {Artificial Intelligence (cs.AI), Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Mathematics, Machine Learning (cs.LG), Machine Learning (stat.ML), Spectral Theory (math.SP)}
}

@article{bandan_enhancing_2024,
  title      = {Enhancing {Fashion} {Choices}: {AI}-{Powered} {Style} {Analysis} and {Recommendations}},
  volume     = {IX},
  issn       = {24546194},
  shorttitle = {Enhancing {Fashion} {Choices}},
  url        = {https://rsisinternational.org/journals/ijrias/articles/enhancing-fashion-choices-ai-powered-style-analysis-and-recommendations/},
  doi        = {10.51584/IJRIAS.2024.908042},
  abstract   = {Fashion has always been an essential feature of our daily routine. It plays an important role in everyone’s life. The online fashion market continues to grow, and an algorithm capable of identifying clothing can help companies in the apparel industry understand the profile of potential buyers and focus sales on specific niches. Artificial intelligence capable of understanding, recommending and labeling human clothing is essential, and can be used to improve sales or better understand users. In this paper, we used our own generated dataset, where the total number of data was 1000. The dataset contains total 10 categories such as shirt, punjabi, t-shirt, blazer, sweater, saree, salwar kameez, gown, western tops and party wear. All the data we have collected from online like social media, google, facebook, instagram, linkedin. The topic combines the fields of fashion, style and machine learning to create a system that can analyze fashion images, classifying them into different styles. In this paper I have used the Customize CNN Algorithm, through which we have used the 7 architectures of CNN. The 7 custom CNN methods we used are MobileNetV2, MobileNetV3, EfficientNet B0, EfficientNet B3, Inception V3, DenseNet201 and VGG19. Here we can see that the accuracy of MobileNetV2 is 59\%, the accuracy of MobileNetV3 is 75\%, the accuracy of EfficientNet B0 is 80\%, the accuracy of EfficientNet B3 is 86\%, the accuracy of Inception V3 is 60\%, the accuracy of DenseNet201 is 65\% and the accuracy of VGG19 is 85\%.},
  number     = {VIII},
  urldate    = {2025-04-16},
  journal    = {International Journal of Research and Innovation in Applied Science},
  author     = {Bandan., Sheikh Sadi and Islam Sabbir., Md. Samiul and Hossain., Md Sharuf and Kobra, Khadiza Tul},
  year       = {2024},
  pages      = {478--490}
}

@inproceedings{li_hierarchical_2020,
  address   = {Virtual Event China},
  title     = {Hierarchical {Fashion} {Graph} {Network} for {Personalized} {Outfit} {Recommendation}},
  isbn      = {9781450380164},
  url       = {https://dl.acm.org/doi/10.1145/3397271.3401080},
  doi       = {10.1145/3397271.3401080},
  language  = {en},
  urldate   = {2025-04-16},
  booktitle = {Proceedings of the 43rd {International} {ACM} {SIGIR} {Conference} on {Research} and {Development} in {Information} {Retrieval}},
  publisher = {ACM},
  author    = {Li, Xingchen and Wang, Xiang and He, Xiangnan and Chen, Long and Xiao, Jun and Chua, Tat-Seng},
  month     = jul,
  year      = {2020},
  pages     = {159--168}
}

@article{shi_generative_2025,
  title      = {Generative {AI} in {Fashion}: {Overview}},
  issn       = {2157-6904, 2157-6912},
  shorttitle = {Generative {AI} in {Fashion}},
  url        = {https://dl.acm.org/doi/10.1145/3718098},
  doi        = {10.1145/3718098},
  abstract   = {Generative Artificial Intelligence (GenAI) has recently gained immense popularity by offering various applications for generating high-quality and aesthetically pleasing content of image, 3D, and video data format. The innovative GenAI solutions have shifted paradigms across various design-related industries, particularly fashion. In this paper, we explore the incorporation of GenAI into fashion-related tasks and applications. Our examination encompasses a thorough review of more than 470 research papers and an in-depth analysis of over 300 applications, focusing on their contributions to the field. These contributions are identified as 13 tasks within four categories: multi-modal fashion understanding, and fashion synthesis of image, 3D, and dynamic (video and animatable 3D) formats We delve into these methods, recognizing their potential to propel future endeavours toward achieving state-of-the-art (SOTA) performance. Furthermore, we present a comprehensive overview of 53 publicly available datasets suitable for training and benchmarking fashion-centric models, accompanied by the relevant evaluation metrics. Finally, we review real-world applications, unveiling existing challenges and future directions. With comprehensive investigation and in-depth analysis, this paper is targeted to serve as a useful resource for understanding the current landscape of GenAI in fashion, paving the way for future innovations in this dynamic field. Papers discussed in this paper, along with public code and datasets links are available at: 
                https://github.com/wendashi/Cool-GenAI-Fashion-Papers/ 
                .},
  language   = {en},
  urldate    = {2025-04-16},
  journal    = {ACM Transactions on Intelligent Systems and Technology},
  author     = {Shi, Wenda and Wong, Waikeung and Zou, Xingxing},
  month      = feb,
  year       = {2025},
  pages      = {3718098}
}

@article{kouslis_ai_2024,
  title      = {{AI} in fashion: a literature review},
  issn       = {1389-5753, 1572-9362},
  shorttitle = {{AI} in fashion},
  url        = {https://link.springer.com/10.1007/s10660-024-09872-z},
  doi        = {10.1007/s10660-024-09872-z},
  language   = {en},
  urldate    = {2025-04-16},
  journal    = {Electronic Commerce Research},
  author     = {Kouslis, Elias and Papachristou, Evridiki and Stavropoulos, Thanos G. and Papazoglou Chalikias, Anastasios and Chatzilari, Elisavet and Nikolopoulos, Spiros and Kompatsiaris, Ioannis},
  month      = jun,
  year       = {2024}
}

@inproceedings{huang_outfit_2017,
  address   = {Wuhan, China},
  title     = {Outfit {Recommendation} {System} {Based} on {Deep} {Learning}},
  isbn      = {9789462523616},
  url       = {http://www.atlantis-press.com/php/paper-details.php?id=25880175},
  doi       = {10.2991/iccia-17.2017.26},
  language  = {en},
  urldate   = {2025-04-04},
  booktitle = {Proceedings of the 2nd {International} {Conference} on {Computer} {Engineering}, {Information} {Science} \& {Application} {Technology} ({ICCIA} 2017)},
  publisher = {Atlantis Press},
  author    = {Huang, Ying and Huang, Tao},
  year      = {2017}
}

@incollection{de_rosa_learning_2022,
  title      = {Learning to weight similarity measures with {Siamese} networks: a case study on optimum-path forest},
  copyright  = {https://www.elsevier.com/tdm/userlicense/1.0/},
  isbn       = {9780128226889},
  shorttitle = {Learning to weight similarity measures with {Siamese} networks},
  url        = {https://linkinghub.elsevier.com/retrieve/pii/B9780128226889000153},
  language   = {en},
  urldate    = {2025-03-05},
  booktitle  = {Optimum-{Path} {Forest}},
  publisher  = {Elsevier},
  author     = {De Rosa, Gustavo H. and Papa, João Paulo},
  year       = {2022},
  doi        = {10.1016/B978-0-12-822688-9.00015-3},
  pages      = {155--173}
}
